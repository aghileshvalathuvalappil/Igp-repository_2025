{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf926d84-4935-41f8-b321-cb940dbbc4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Dataset shape: (202, 33)\n",
      "\n",
      "First 5 rows:\n",
      "  Column 1 Date of experiment  \\\n",
      "0  AXF0020         2024-07-24   \n",
      "1  AXF0021         2024-07-30   \n",
      "2  AXF0022         2024-08-02   \n",
      "3  AXF0023         2024-08-09   \n",
      "4  AXF0024         2024-08-09   \n",
      "\n",
      "                                   Aims & Hypothesis Core used  \\\n",
      "0                                       Baseline run  BNC00017   \n",
      "1  Does removing PEDGA from UV remove the 'wickin...  BNC00022   \n",
      "2  Repeat of AXF0021, at smaller scale to create ...  BNC00017   \n",
      "3   Size screen as a function of dispersed flow rate  BNC00017   \n",
      "4   Size screen as a function of dispersed flow rate  BNC00017   \n",
      "\n",
      "  Core Formulation  Core Viscosity (cP)   UV used UV formulation  \\\n",
      "0     OG (66/27/7)               4560.0  BNU00015          AUF69   \n",
      "1     OG (66/27/7)               6180.0  BNU00016          AUF85   \n",
      "2     OG (66/27/7)               6852.0  BNU00017          AUF85   \n",
      "3     OG (66/27/7)               6756.0  BNU00018          AUF85   \n",
      "4     OG (66/27/7)               6756.0  BNU00018          AUF85   \n",
      "\n",
      "   UV viscosity (cP)  Emulsion viscosity (cP)  ...  \\\n",
      "0              228.0                    476.4  ...   \n",
      "1              260.0                    598.8  ...   \n",
      "2              262.8                    598.8  ...   \n",
      "3              247.2                    594.0  ...   \n",
      "4              247.2                    594.0  ...   \n",
      "\n",
      "   Droplet/particle size (µm) Droplet/particle size range  \\\n",
      "0                        43.1                   6.5-116.6   \n",
      "1                        53.3                   9.1-136.6   \n",
      "2                        45.9                  10.5-114.7   \n",
      "3                        59.6                  18.9-126.5   \n",
      "4                        51.6                  24.6-108.8   \n",
      "\n",
      "   Droplet/particle size range STDEV % single core  % empty  \\\n",
      "0                               20.4         23.80     0.08   \n",
      "1                               22.5          5.85     0.02   \n",
      "2                               19.1         25.50     0.04   \n",
      "3                               21.2         23.50     0.00   \n",
      "4                               11.9         47.20     0.00   \n",
      "\n",
      "   Polydispersity Index  % dry after 24h in incubator  \\\n",
      "0              0.224030                           NaN   \n",
      "1              0.178201                          0.95   \n",
      "2              0.173158                          0.95   \n",
      "3              0.126526                          0.80   \n",
      "4              0.053186                          0.60   \n",
      "\n",
      "   % dry after 2 days 70% humidity Transmission window  \\\n",
      "0                              NaN             0.03650   \n",
      "1                              NaN             0.01780   \n",
      "2                              NaN             0.03410   \n",
      "3                              NaN             0.04165   \n",
      "4                              NaN             0.02100   \n",
      "\n",
      "                                           Notes  \n",
      "0                                            NaN  \n",
      "1                                            NaN  \n",
      "2                                            NaN  \n",
      "3  No nail lamp in cure rig. UV tube lights only  \n",
      "4  No nail lamp in cure rig. UV tube lights only  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "\n",
      "Column information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 202 entries, 0 to 201\n",
      "Data columns (total 33 columns):\n",
      " #   Column                             Non-Null Count  Dtype         \n",
      "---  ------                             --------------  -----         \n",
      " 0   Column 1                           202 non-null    object        \n",
      " 1   Date of experiment                 202 non-null    datetime64[ns]\n",
      " 2   Aims & Hypothesis                  201 non-null    object        \n",
      " 3   Core used                          202 non-null    object        \n",
      " 4   Core Formulation                   202 non-null    object        \n",
      " 5   Core Viscosity (cP)                193 non-null    float64       \n",
      " 6   UV used                            202 non-null    object        \n",
      " 7   UV formulation                     202 non-null    object        \n",
      " 8   UV viscosity (cP)                  197 non-null    float64       \n",
      " 9   Emulsion viscosity (cP)            193 non-null    float64       \n",
      " 10  Vessel size (mL)                   202 non-null    int64         \n",
      " 11  Impellor                           202 non-null    object        \n",
      " 12  Spin Speed (rpm)                   202 non-null    int64         \n",
      " 13  Outer used                         202 non-null    object        \n",
      " 14  Outer viscosity (cP)               200 non-null    float64       \n",
      " 15  Dispersed Flow Rate (mL/min)       202 non-null    float64       \n",
      " 16  Continuous Flow Rate (mL/min)      202 non-null    int64         \n",
      " 17  Flow Rate Ratio                    202 non-null    float64       \n",
      " 18  Cure Rig Used                      202 non-null    object        \n",
      " 19  Length of curing tubing (m)        202 non-null    int64         \n",
      " 20  Temperature of Outer Phase         91 non-null     float64       \n",
      " 21  UV Power (J s-1)                   201 non-null    float64       \n",
      " 22  Curing Energy (kJ g-1)             201 non-null    float64       \n",
      " 23  Droplet/particle size (µm)         91 non-null     float64       \n",
      " 24  Droplet/particle size range        88 non-null     object        \n",
      " 25  Droplet/particle size range STDEV  57 non-null     float64       \n",
      " 26  % single core                      71 non-null     float64       \n",
      " 27  % empty                            71 non-null     float64       \n",
      " 28  Polydispersity Index               57 non-null     float64       \n",
      " 29  % dry after 24h in incubator       175 non-null    float64       \n",
      " 30  % dry after 2 days 70% humidity    169 non-null    float64       \n",
      " 31  Transmission window                41 non-null     float64       \n",
      " 32  Notes                              61 non-null     object        \n",
      "dtypes: datetime64[ns](1), float64(17), int64(4), object(11)\n",
      "memory usage: 52.2+ KB\n",
      "None\n",
      "\n",
      "Summary statistics:\n",
      "                  Date of experiment  Core Viscosity (cP)  UV viscosity (cP)  \\\n",
      "count                            202           193.000000         197.000000   \n",
      "mean   2024-12-09 00:07:07.722772224         12857.305699         405.988832   \n",
      "min              2024-07-24 00:00:00          4560.000000          75.600000   \n",
      "25%              2024-11-07 06:00:00          9036.000000         255.600000   \n",
      "50%              2024-12-09 00:00:00         11000.000000         294.000000   \n",
      "75%              2025-01-30 00:00:00         12270.000000         375.600000   \n",
      "max              2025-02-20 00:00:00         86400.000000        4860.000000   \n",
      "std                              NaN          9946.297070         400.787516   \n",
      "\n",
      "       Emulsion viscosity (cP)  Vessel size (mL)  Spin Speed (rpm)  \\\n",
      "count               193.000000        202.000000        202.000000   \n",
      "mean                826.829016       3279.207921        235.579208   \n",
      "min                 164.400000        600.000000        100.000000   \n",
      "25%                 530.400000        600.000000        112.000000   \n",
      "50%                 592.800000       5000.000000        125.000000   \n",
      "75%                 705.600000       5000.000000        275.000000   \n",
      "max               13460.000000       5000.000000       1050.000000   \n",
      "std                1020.530133       2152.509438        222.914383   \n",
      "\n",
      "       Outer viscosity (cP)  Dispersed Flow Rate (mL/min)  \\\n",
      "count            200.000000                    202.000000   \n",
      "mean              12.014750                     30.731436   \n",
      "min                8.400000                     10.000000   \n",
      "25%               10.800000                     20.000000   \n",
      "50%               12.000000                     30.000000   \n",
      "75%               13.200000                     30.000000   \n",
      "max               15.600000                    100.000000   \n",
      "std                1.582344                     16.271281   \n",
      "\n",
      "       Continuous Flow Rate (mL/min)  Flow Rate Ratio  ...  UV Power (J s-1)  \\\n",
      "count                     202.000000       202.000000  ...        201.000000   \n",
      "mean                      271.138614        11.210264  ...        636.129353   \n",
      "min                        50.000000         2.500000  ...        108.000000   \n",
      "25%                       150.000000         5.000000  ...        488.000000   \n",
      "50%                       300.000000        10.000000  ...        708.000000   \n",
      "75%                       300.000000        10.000000  ...        708.000000   \n",
      "max                       900.000000        30.000000  ...       1032.000000   \n",
      "std                       135.034326         8.033389  ...        235.646012   \n",
      "\n",
      "       Curing Energy (kJ g-1)  Droplet/particle size (µm)  \\\n",
      "count              201.000000                   91.000000   \n",
      "mean                 1.721468                   67.765934   \n",
      "min                  0.025920                   43.100000   \n",
      "25%                  0.868000                   55.350000   \n",
      "50%                  1.557600                   64.100000   \n",
      "75%                  1.840800                   75.500000   \n",
      "max                  5.469600                  139.400000   \n",
      "std                  1.319003                   17.294406   \n",
      "\n",
      "       Droplet/particle size range STDEV  % single core    % empty  \\\n",
      "count                          57.000000      71.000000  71.000000   \n",
      "mean                           20.739649      17.464930   0.060968   \n",
      "min                            11.200000       0.000000   0.000000   \n",
      "25%                            15.500000       6.450000   0.013900   \n",
      "50%                            19.100000      19.700000   0.050000   \n",
      "75%                            23.100000      25.000000   0.093000   \n",
      "max                            49.400000      52.400000   0.400000   \n",
      "std                             7.211823      11.927736   0.062324   \n",
      "\n",
      "       Polydispersity Index  % dry after 24h in incubator  \\\n",
      "count             57.000000                    175.000000   \n",
      "mean               0.118233                      0.439371   \n",
      "min                0.025189                      0.020000   \n",
      "25%                0.070601                      0.150000   \n",
      "50%                0.119339                      0.300000   \n",
      "75%                0.143998                      0.700000   \n",
      "max                0.285214                      1.000000   \n",
      "std                0.054690                      0.325847   \n",
      "\n",
      "       % dry after 2 days 70% humidity  Transmission window  \n",
      "count                       169.000000            41.000000  \n",
      "mean                          0.558817             0.033930  \n",
      "min                           0.050000             0.010000  \n",
      "25%                           0.200000             0.022000  \n",
      "50%                           0.500000             0.032200  \n",
      "75%                           1.000000             0.040100  \n",
      "max                           1.000000             0.079000  \n",
      "std                           0.356566             0.015688  \n",
      "\n",
      "[8 rows x 22 columns]\n",
      "\n",
      "Missing values by column:\n",
      "                                   Missing Values  Percentage\n",
      "Transmission window                           161   79.702970\n",
      "Droplet/particle size range STDEV             145   71.782178\n",
      "Polydispersity Index                          145   71.782178\n",
      "Notes                                         141   69.801980\n",
      "% empty                                       131   64.851485\n",
      "% single core                                 131   64.851485\n",
      "Droplet/particle size range                   114   56.435644\n",
      "Droplet/particle size (µm)                    111   54.950495\n",
      "Temperature of Outer Phase                    111   54.950495\n",
      "% dry after 2 days 70% humidity                33   16.336634\n",
      "% dry after 24h in incubator                   27   13.366337\n",
      "Core Viscosity (cP)                             9    4.455446\n",
      "Emulsion viscosity (cP)                         9    4.455446\n",
      "UV viscosity (cP)                               5    2.475248\n",
      "Outer viscosity (cP)                            2    0.990099\n",
      "Curing Energy (kJ g-1)                          1    0.495050\n",
      "UV Power (J s-1)                                1    0.495050\n",
      "Aims & Hypothesis                               1    0.495050\n",
      "\n",
      "Target variable '% dry after 24h in incubator' statistics:\n",
      "count    175.000000\n",
      "mean       0.439371\n",
      "std        0.325847\n",
      "min        0.020000\n",
      "25%        0.150000\n",
      "50%        0.300000\n",
      "75%        0.700000\n",
      "max        1.000000\n",
      "Name: % dry after 24h in incubator, dtype: float64\n",
      "Target skewness: 0.5398413954341804\n",
      "\n",
      "Excluding metadata columns: ['Column 1', 'Date of experiment', 'Aims & Hypothesis', 'Notes']\n",
      "Excluding outcome columns: ['% dry after 24h in incubator', '% dry after 2 days 70% humidity', 'Droplet/particle size (µm)', 'Droplet/particle size range', 'Droplet/particle size range STDEV', '% single core', '% empty', 'Polydispersity Index', 'Transmission window']\n",
      "Excluding multicollinear columns: ['UV viscosity (cP)']\n",
      "Excluding high missing value columns: ['Temperature of Outer Phase']\n",
      "Using 18 columns for analysis:\n",
      "['Core used', 'Core Formulation', 'Core Viscosity (cP)', 'UV used', 'UV formulation', 'Emulsion viscosity (cP)', 'Vessel size (mL)', 'Impellor', 'Spin Speed (rpm)', 'Outer used', 'Outer viscosity (cP)', 'Dispersed Flow Rate (mL/min)', 'Continuous Flow Rate (mL/min)', 'Flow Rate Ratio', 'Cure Rig Used', 'Length of curing tubing (m)', 'UV Power (J s-1)', 'Curing Energy (kJ g-1)']\n",
      "\n",
      "Analyzing missing value patterns...\n",
      "\n",
      "Columns with low missing percentage (<10%): 18\n",
      "  - Core used: 0.00%\n",
      "  - Core Formulation: 0.00%\n",
      "  - Core Viscosity (cP): 4.46%\n",
      "  - UV used: 0.00%\n",
      "  - UV formulation: 0.00%\n",
      "  - Emulsion viscosity (cP): 4.46%\n",
      "  - Vessel size (mL): 0.00%\n",
      "  - Impellor: 0.00%\n",
      "  - Spin Speed (rpm): 0.00%\n",
      "  - Outer used: 0.00%\n",
      "  - Outer viscosity (cP): 0.99%\n",
      "  - Dispersed Flow Rate (mL/min): 0.00%\n",
      "  - Continuous Flow Rate (mL/min): 0.00%\n",
      "  - Flow Rate Ratio: 0.00%\n",
      "  - Cure Rig Used: 0.00%\n",
      "  - Length of curing tubing (m): 0.00%\n",
      "  - UV Power (J s-1): 0.50%\n",
      "  - Curing Energy (kJ g-1): 0.50%\n",
      "\n",
      "Columns with medium missing percentage (10-30%): 0\n",
      "\n",
      "Columns with high missing percentage (30-50%): 0\n",
      "\n",
      "Analyzing categorical features...\n",
      "Relevant categorical columns: ['Core used', 'Core Formulation', 'UV used', 'UV formulation', 'Impellor', 'Outer used', 'Cure Rig Used']\n",
      "\n",
      "Analyzing Core used:\n",
      "Number of unique values: 25\n",
      "Top 5 most common values:\n",
      "Core used\n",
      "BNC00044    36\n",
      "BNC00045    30\n",
      "BNC00036    24\n",
      "BNC00041    18\n",
      "BNC00035    15\n",
      "Name: count, dtype: int64\n",
      "WARNING: Column Core used has 25 unique values. May cause overfitting with one-hot encoding.\n",
      "ANOVA for Core used: F-statistic = 11.1350, p-value = 0.0000\n",
      "\n",
      "Analyzing Core Formulation:\n",
      "Number of unique values: 9\n",
      "Top 5 most common values:\n",
      "Core Formulation\n",
      "OG (66/27/7)                          179\n",
      "66/26/6/2% PEG550DMA                    8\n",
      "94/6 (25%)                              5\n",
      "66/25.5/6.5/2% PEG550DMA                3\n",
      "64/24/10/2 HPC/PDO/CaCl2/PEG550DMA      2\n",
      "Name: count, dtype: int64\n",
      "ANOVA for Core Formulation: F-statistic = 3.5693, p-value = 0.0080\n",
      "\n",
      "Analyzing UV used:\n",
      "Number of unique values: 60\n",
      "Top 5 most common values:\n",
      "UV used\n",
      "BNU00074    20\n",
      "BNU00044    18\n",
      "BNU00068    14\n",
      "BNU00070    11\n",
      "BNU00069    11\n",
      "Name: count, dtype: int64\n",
      "WARNING: Column UV used has 60 unique values. May cause overfitting with one-hot encoding.\n",
      "ANOVA for UV used: F-statistic = 92.4860, p-value = 0.0000\n",
      "\n",
      "Analyzing UV formulation:\n",
      "Number of unique values: 27\n",
      "Top 5 most common values:\n",
      "UV formulation\n",
      "AUF76     77\n",
      "AUF110    24\n",
      "AUF85     19\n",
      "AUF93     14\n",
      "AUF138    13\n",
      "Name: count, dtype: int64\n",
      "WARNING: Column UV formulation has 27 unique values. May cause overfitting with one-hot encoding.\n",
      "ANOVA for UV formulation: F-statistic = 61.2164, p-value = 0.0000\n",
      "\n",
      "Analyzing Impellor:\n",
      "Number of unique values: 3\n",
      "Top 5 most common values:\n",
      "Impellor\n",
      "Impellorpellor        122\n",
      "Medium 4x impellor     79\n",
      "Large 4x impellor       1\n",
      "Name: count, dtype: int64\n",
      "ANOVA for Impellor: F-statistic = 3.4495, p-value = 0.0340\n",
      "\n",
      "Analyzing Outer used:\n",
      "Number of unique values: 79\n",
      "Top 5 most common values:\n",
      "Outer used\n",
      "BNO00104    11\n",
      "BNO00106    10\n",
      "BNO00100    10\n",
      "BNO00102    10\n",
      "BNO00110     9\n",
      "Name: count, dtype: int64\n",
      "WARNING: Column Outer used has 79 unique values. May cause overfitting with one-hot encoding.\n",
      "ANOVA for Outer used: F-statistic = 92.1489, p-value = 0.0000\n",
      "\n",
      "Analyzing Cure Rig Used:\n",
      "Number of unique values: 3\n",
      "Top 5 most common values:\n",
      "Cure Rig Used\n",
      "Cure Rig 1.0 (Water Butt)             113\n",
      "Cure Rig 2.0 (Upgraded Water Butt)     77\n",
      "Horizontal OG Cure Rig                 12\n",
      "Name: count, dtype: int64\n",
      "ANOVA for Cure Rig Used: F-statistic = 2.3422, p-value = 0.0992\n",
      "\n",
      "Analyzing numerical features...\n",
      "Relevant numerical columns: ['Core Viscosity (cP)', 'Emulsion viscosity (cP)', 'Vessel size (mL)', 'Spin Speed (rpm)', 'Outer viscosity (cP)', 'Dispersed Flow Rate (mL/min)', 'Continuous Flow Rate (mL/min)', 'Flow Rate Ratio', 'Length of curing tubing (m)', 'UV Power (J s-1)', 'Curing Energy (kJ g-1)']\n",
      "\n",
      "Correlations with target variable:\n",
      "% dry after 24h in incubator     1.000000\n",
      "Emulsion viscosity (cP)          0.232042\n",
      "Length of curing tubing (m)      0.050159\n",
      "Spin Speed (rpm)                 0.046306\n",
      "Dispersed Flow Rate (mL/min)     0.014537\n",
      "Flow Rate Ratio                  0.012991\n",
      "Outer viscosity (cP)            -0.004063\n",
      "Continuous Flow Rate (mL/min)   -0.004906\n",
      "Curing Energy (kJ g-1)          -0.008799\n",
      "UV Power (J s-1)                -0.098087\n",
      "Core Viscosity (cP)             -0.112989\n",
      "Vessel size (mL)                -0.148899\n",
      "Name: % dry after 24h in incubator, dtype: float64\n",
      "\n",
      "Excluding high-cardinality categorical columns to avoid overfitting: ['Core used', 'UV used', 'UV formulation', 'Outer used']\n",
      "Using 3 categorical columns for modeling\n",
      "\n",
      "Performing feature engineering...\n",
      "\n",
      "Creating interaction features for top correlated features: ['Emulsion viscosity (cP)', 'Dispersed Flow Rate (mL/min)', 'Core Viscosity (cP)', 'Vessel size (mL)', 'UV Power (J s-1)']\n",
      "Created 24 interaction and physical features\n",
      "Created 6 polynomial features\n",
      "Created 2 binned features\n",
      "Created 8 transformed features\n",
      "\n",
      "Designing imputation strategy...\n",
      "\n",
      "Imputation strategy:\n",
      "- Simple median imputation for numerical columns with <10% missing values\n",
      "- KNN imputation for numerical columns with 10-30% missing values\n",
      "- KNN imputation with more neighbors for columns with >30% missing values\n",
      "- Most frequent value imputation for categorical columns\n",
      "- Missing value indicators will be created for 0 columns\n",
      "\n",
      "Preparing data for modeling...\n",
      "Data after removing rows with missing target: (175, 56)\n",
      "\n",
      "Handling outliers in numerical features...\n",
      "Capping outliers in Core Viscosity (cP) using IQR method\n",
      "  - Lower bound: 3551.25\n",
      "  - Upper bound: 18177.25\n",
      "  - Outliers found: 18 (10.84%)\n",
      "Capping outliers in UV viscosity (cP) using IQR method\n",
      "  - Lower bound: 75.60\n",
      "  - Upper bound: 555.60\n",
      "  - Outliers found: 25 (14.62%)\n",
      "Capping outliers in Emulsion viscosity (cP) using IQR method\n",
      "  - Lower bound: 256.20\n",
      "  - Upper bound: 961.80\n",
      "  - Outliers found: 31 (18.56%)\n",
      "Capping outliers in Vessel size (mL) using IQR method\n",
      "  - Lower bound: -6000.00\n",
      "  - Upper bound: 11600.00\n",
      "  - Outliers found: 0 (0.00%)\n",
      "Capping outliers in Spin Speed (rpm) using IQR method\n",
      "  - Lower bound: -132.50\n",
      "  - Upper bound: 519.50\n",
      "  - Outliers found: 16 (9.14%)\n",
      "Capping outliers in Outer viscosity (cP) using IQR method\n",
      "  - Lower bound: 7.20\n",
      "  - Upper bound: 16.80\n",
      "  - Outliers found: 0 (0.00%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min) using IQR method\n",
      "  - Lower bound: 5.00\n",
      "  - Upper bound: 45.00\n",
      "  - Outliers found: 30 (17.14%)\n",
      "Capping outliers in Continuous Flow Rate (mL/min) using IQR method\n",
      "  - Lower bound: -75.00\n",
      "  - Upper bound: 525.00\n",
      "  - Outliers found: 12 (6.86%)\n",
      "Capping outliers in Flow Rate Ratio using IQR method\n",
      "  - Lower bound: -2.50\n",
      "  - Upper bound: 17.50\n",
      "  - Outliers found: 26 (14.86%)\n",
      "Capping outliers in Length of curing tubing (m) using IQR method\n",
      "  - Lower bound: 50.00\n",
      "  - Upper bound: 50.00\n",
      "  - Outliers found: 21 (12.00%)\n",
      "Skipping Temperature of Outer Phase for outlier detection due to high missing percentage\n",
      "Capping outliers in UV Power (J s-1) using IQR method\n",
      "  - Lower bound: 158.00\n",
      "  - Upper bound: 1038.00\n",
      "  - Outliers found: 8 (4.57%)\n",
      "Capping outliers in Curing Energy (kJ g-1) using IQR method\n",
      "  - Lower bound: -0.68\n",
      "  - Upper bound: 3.48\n",
      "  - Outliers found: 27 (15.43%)\n",
      "Capping outliers in Emulsion viscosity (cP)_x_Dispersed Flow Rate (mL/min) using IQR method\n",
      "  - Lower bound: -6825.00\n",
      "  - Upper bound: 44655.00\n",
      "  - Outliers found: 6 (3.59%)\n",
      "Capping outliers in Emulsion viscosity (cP)_to_Dispersed Flow Rate (mL/min) using IQR method\n",
      "  - Lower bound: -31.21\n",
      "  - Upper bound: 93.18\n",
      "  - Outliers found: 7 (4.19%)\n",
      "Capping outliers in Emulsion viscosity (cP)_x_Core Viscosity (cP) using IQR method\n",
      "  - Lower bound: -5706771.00\n",
      "  - Upper bound: 22161285.00\n",
      "  - Outliers found: 13 (8.23%)\n",
      "Capping outliers in Emulsion viscosity (cP)_to_Core Viscosity (cP) using IQR method\n",
      "  - Lower bound: -0.01\n",
      "  - Upper bound: 0.13\n",
      "  - Outliers found: 20 (12.66%)\n",
      "Capping outliers in Emulsion viscosity (cP)_x_Vessel size (mL) using IQR method\n",
      "  - Lower bound: -3488400.00\n",
      "  - Upper bound: 6873840.00\n",
      "  - Outliers found: 15 (8.98%)\n",
      "Capping outliers in Emulsion viscosity (cP)_to_Vessel size (mL) using IQR method\n",
      "  - Lower bound: -1.12\n",
      "  - Upper bound: 2.15\n",
      "  - Outliers found: 11 (6.59%)\n",
      "Capping outliers in Emulsion viscosity (cP)_x_UV Power (J s-1) using IQR method\n",
      "  - Lower bound: 45751.20\n",
      "  - Upper bound: 727844.00\n",
      "  - Outliers found: 19 (11.38%)\n",
      "Capping outliers in Emulsion viscosity (cP)_to_UV Power (J s-1) using IQR method\n",
      "  - Lower bound: -0.47\n",
      "  - Upper bound: 2.75\n",
      "  - Outliers found: 20 (11.98%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_x_Core Viscosity (cP) using IQR method\n",
      "  - Lower bound: -75275.00\n",
      "  - Upper bound: 764925.00\n",
      "  - Outliers found: 18 (10.84%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_to_Core Viscosity (cP) using IQR method\n",
      "  - Lower bound: -0.00\n",
      "  - Upper bound: 0.01\n",
      "  - Outliers found: 5 (3.01%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_x_Vessel size (mL) using IQR method\n",
      "  - Lower bound: -180000.00\n",
      "  - Upper bound: 348000.00\n",
      "  - Outliers found: 6 (3.43%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_to_Vessel size (mL) using IQR method\n",
      "  - Lower bound: -0.01\n",
      "  - Upper bound: 0.03\n",
      "  - Outliers found: 42 (24.00%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_x_UV Power (J s-1) using IQR method\n",
      "  - Lower bound: -8017.00\n",
      "  - Upper bound: 42586.20\n",
      "  - Outliers found: 10 (5.71%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_to_UV Power (J s-1) using IQR method\n",
      "  - Lower bound: -0.03\n",
      "  - Upper bound: 0.13\n",
      "  - Outliers found: 17 (9.71%)\n",
      "Capping outliers in Core Viscosity (cP)_x_Vessel size (mL) using IQR method\n",
      "  - Lower bound: -63900000.00\n",
      "  - Upper bound: 126340000.00\n",
      "  - Outliers found: 0 (0.00%)\n",
      "Capping outliers in Core Viscosity (cP)_to_Vessel size (mL) using IQR method\n",
      "  - Lower bound: -20.29\n",
      "  - Upper bound: 39.10\n",
      "  - Outliers found: 10 (6.02%)\n",
      "Capping outliers in Core Viscosity (cP)_x_UV Power (J s-1) using IQR method\n",
      "  - Lower bound: -717245.00\n",
      "  - Upper bound: 14982075.00\n",
      "  - Outliers found: 13 (7.83%)\n",
      "Capping outliers in Core Viscosity (cP)_to_UV Power (J s-1) using IQR method\n",
      "  - Lower bound: -9.63\n",
      "  - Upper bound: 50.33\n",
      "  - Outliers found: 13 (7.83%)\n",
      "Capping outliers in Vessel size (mL)_x_UV Power (J s-1) using IQR method\n",
      "  - Lower bound: -4248000.00\n",
      "  - Upper bound: 8212800.00\n",
      "  - Outliers found: 0 (0.00%)\n",
      "Capping outliers in Vessel size (mL)_to_UV Power (J s-1) using IQR method\n",
      "  - Lower bound: -8.50\n",
      "  - Upper bound: 16.43\n",
      "  - Outliers found: 11 (6.29%)\n",
      "Capping outliers in Flow_Ratio using IQR method\n",
      "  - Lower bound: -0.05\n",
      "  - Upper bound: 0.35\n",
      "  - Outliers found: 11 (6.29%)\n",
      "Capping outliers in Energy_Density using IQR method\n",
      "  - Lower bound: -15.02\n",
      "  - Upper bound: 62.37\n",
      "  - Outliers found: 23 (13.14%)\n",
      "Capping outliers in Residence_Time using IQR method\n",
      "  - Lower bound: 0.04\n",
      "  - Upper bound: 0.38\n",
      "  - Outliers found: 29 (16.57%)\n",
      "Capping outliers in Viscosity_Ratio using IQR method\n",
      "  - Lower bound: -0.01\n",
      "  - Upper bound: 0.13\n",
      "  - Outliers found: 20 (12.66%)\n",
      "Capping outliers in Emulsion viscosity (cP)_squared using IQR method\n",
      "  - Lower bound: -51050.16\n",
      "  - Upper bound: 808370.64\n",
      "  - Outliers found: 29 (17.37%)\n",
      "Capping outliers in Emulsion viscosity (cP)_sqrt using IQR method\n",
      "  - Lower bound: 17.45\n",
      "  - Upper bound: 31.78\n",
      "  - Outliers found: 31 (18.56%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_squared using IQR method\n",
      "  - Lower bound: -350.00\n",
      "  - Upper bound: 1650.00\n",
      "  - Outliers found: 31 (17.71%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_sqrt using IQR method\n",
      "  - Lower bound: 2.96\n",
      "  - Upper bound: 6.98\n",
      "  - Outliers found: 30 (17.14%)\n",
      "Capping outliers in Core Viscosity (cP)_squared using IQR method\n",
      "  - Lower bound: -37568872.50\n",
      "  - Upper bound: 280346243.50\n",
      "  - Outliers found: 18 (10.84%)\n",
      "Capping outliers in Core Viscosity (cP)_sqrt using IQR method\n",
      "  - Lower bound: 68.66\n",
      "  - Upper bound: 139.06\n",
      "  - Outliers found: 15 (9.04%)\n",
      "Capping outliers in Emulsion viscosity (cP)_binned using IQR method\n",
      "  - Lower bound: -2.00\n",
      "  - Upper bound: 6.00\n",
      "  - Outliers found: 0 (0.00%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_binned using IQR method\n",
      "  - Lower bound: -1.50\n",
      "  - Upper bound: 2.50\n",
      "  - Outliers found: 31 (17.71%)\n",
      "Capping outliers in Core Viscosity (cP)_log using IQR method\n",
      "  - Lower bound: 8.60\n",
      "  - Upper bound: 9.96\n",
      "  - Outliers found: 14 (8.43%)\n",
      "Capping outliers in Emulsion viscosity (cP)_log using IQR method\n",
      "  - Lower bound: 5.82\n",
      "  - Upper bound: 6.99\n",
      "  - Outliers found: 31 (18.56%)\n",
      "Capping outliers in Spin Speed (rpm)_log using IQR method\n",
      "  - Lower bound: 3.39\n",
      "  - Upper bound: 6.96\n",
      "  - Outliers found: 0 (0.00%)\n",
      "Capping outliers in Dispersed Flow Rate (mL/min)_log using IQR method\n",
      "  - Lower bound: 2.46\n",
      "  - Upper bound: 4.02\n",
      "  - Outliers found: 45 (25.71%)\n",
      "Capping outliers in Continuous Flow Rate (mL/min)_log using IQR method\n",
      "  - Lower bound: 3.98\n",
      "  - Upper bound: 6.74\n",
      "  - Outliers found: 2 (1.14%)\n",
      "Capping outliers in Flow Rate Ratio_log using IQR method\n",
      "  - Lower bound: 0.88\n",
      "  - Upper bound: 3.31\n",
      "  - Outliers found: 23 (13.14%)\n",
      "Capping outliers in Length of curing tubing (m)_log using IQR method\n",
      "  - Lower bound: 3.93\n",
      "  - Upper bound: 3.93\n",
      "  - Outliers found: 21 (12.00%)\n",
      "Capping outliers in Curing Energy (kJ g-1)_log using IQR method\n",
      "  - Lower bound: -0.03\n",
      "  - Upper bound: 1.73\n",
      "  - Outliers found: 2 (1.14%)\n",
      "Imputing Core Viscosity (cP) using median strategy\n",
      "Imputing Emulsion viscosity (cP) using median strategy\n",
      "Imputing Outer viscosity (cP) using median strategy\n",
      "WARNING: 312 missing values remain after imputation\n",
      "Columns with remaining missing values: ['UV viscosity (cP)', 'Temperature of Outer Phase', 'Emulsion viscosity (cP)_x_Dispersed Flow Rate (mL/min)', 'Emulsion viscosity (cP)_to_Dispersed Flow Rate (mL/min)', 'Emulsion viscosity (cP)_x_Core Viscosity (cP)', 'Emulsion viscosity (cP)_to_Core Viscosity (cP)', 'Emulsion viscosity (cP)_x_Vessel size (mL)', 'Emulsion viscosity (cP)_to_Vessel size (mL)', 'Emulsion viscosity (cP)_x_UV Power (J s-1)', 'Emulsion viscosity (cP)_to_UV Power (J s-1)', 'Dispersed Flow Rate (mL/min)_x_Core Viscosity (cP)', 'Dispersed Flow Rate (mL/min)_to_Core Viscosity (cP)', 'Core Viscosity (cP)_x_Vessel size (mL)', 'Core Viscosity (cP)_to_Vessel size (mL)', 'Core Viscosity (cP)_x_UV Power (J s-1)', 'Core Viscosity (cP)_to_UV Power (J s-1)', 'Viscosity_Ratio', 'Emulsion viscosity (cP)_squared', 'Emulsion viscosity (cP)_sqrt', 'Core Viscosity (cP)_squared', 'Core Viscosity (cP)_sqrt', 'Emulsion viscosity (cP)_binned', 'Core Viscosity (cP)_log', 'Emulsion viscosity (cP)_log']\n",
      "Training set size: (140, 56)\n",
      "Testing set size: (35, 56)\n",
      "\n",
      "Building preprocessing pipeline with standard scaling...\n",
      "\n",
      "Performing feature selection using importance method...\n",
      "Selected 18 features out of 66\n",
      "Top 10 selected features:\n",
      "                                              Feature  Importance\n",
      "13                                    Viscosity_Ratio    0.131103\n",
      "5      Emulsion viscosity (cP)_to_Core Viscosity (cP)    0.099291\n",
      "0                                   UV viscosity (cP)    0.094562\n",
      "11             Core Viscosity (cP)_x_Vessel size (mL)    0.080611\n",
      "6          Emulsion viscosity (cP)_x_Vessel size (mL)    0.072976\n",
      "4       Emulsion viscosity (cP)_x_Core Viscosity (cP)    0.063338\n",
      "12            Core Viscosity (cP)_to_UV Power (J s-1)    0.036311\n",
      "8          Emulsion viscosity (cP)_x_UV Power (J s-1)    0.026969\n",
      "9   Dispersed Flow Rate (mL/min)_x_Core Viscosity ...    0.025592\n",
      "14                    Emulsion viscosity (cP)_squared    0.025360\n",
      "\n",
      "Performing hyperparameter tuning for all models...\n",
      "\n",
      "Tuning RandomForest...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "RandomForest - Best parameters: {'model__max_depth': 30, 'model__max_features': 'sqrt', 'model__min_samples_leaf': 1, 'model__min_samples_split': 5, 'model__n_estimators': 413}\n",
      "RandomForest - Best CV score: 0.2051 (RMSE)\n",
      "RandomForest - Test RMSE: 0.1916\n",
      "RandomForest - Test R2: 0.6703\n",
      "RandomForest - Test MAE: 0.1431\n",
      "Warning: Could not generate partial dependence plot for Core Viscosity (cP)_to_UV Power (J s-1): 'values'\n",
      "Warning: Could not generate partial dependence plot for Emulsion viscosity (cP)_x_UV Power (J s-1): 'values'\n",
      "Warning: Could not generate partial dependence plot for Emulsion viscosity (cP)_to_UV Power (J s-1): 'values'\n",
      "Warning: Could not generate partial dependence plot for Dispersed Flow Rate (mL/min)_squared: 'values'\n",
      "Warning: Could not generate partial dependence plot for Spin Speed (rpm): 'values'\n",
      "\n",
      "Tuning GradientBoosting...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "GradientBoosting - Best parameters: {'model__learning_rate': 0.07530815376116708, 'model__max_depth': 8, 'model__min_samples_leaf': 3, 'model__min_samples_split': 5, 'model__n_estimators': 289, 'model__subsample': 0.7301321323053057}\n",
      "GradientBoosting - Best CV score: 0.1959 (RMSE)\n",
      "GradientBoosting - Test RMSE: 0.2033\n",
      "GradientBoosting - Test R2: 0.6290\n",
      "GradientBoosting - Test MAE: 0.1504\n",
      "Warning: Could not generate partial dependence plot for Emulsion viscosity (cP)_x_UV Power (J s-1): 'values'\n",
      "Warning: Could not generate partial dependence plot for Core Viscosity (cP)_to_UV Power (J s-1): 'values'\n",
      "Warning: Could not generate partial dependence plot for Dispersed Flow Rate (mL/min)_squared: 'values'\n",
      "Warning: Could not generate partial dependence plot for Emulsion viscosity (cP)_to_UV Power (J s-1): 'values'\n",
      "Warning: Could not generate partial dependence plot for Core Formulation_64/24/10/2 HPC/PDO/CaCl2/PEG550DMA: 'values'\n",
      "\n",
      "Tuning XGBoost...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "XGBoost - Best parameters: {'model__colsample_bytree': 0.8391599915244341, 'model__learning_rate': 0.19437484700462337, 'model__max_depth': 8, 'model__min_child_weight': 7, 'model__n_estimators': 351, 'model__subsample': 0.6180909155642152}\n",
      "XGBoost - Best CV score: 0.1987 (RMSE)\n",
      "XGBoost - Test RMSE: 0.2105\n",
      "XGBoost - Test R2: 0.6021\n",
      "XGBoost - Test MAE: 0.1610\n",
      "Warning: Could not generate partial dependence plot for Emulsion viscosity (cP)_x_UV Power (J s-1): 'values'\n",
      "Warning: Could not generate partial dependence plot for Spin Speed (rpm): 'values'\n",
      "Warning: Could not generate partial dependence plot for Core Viscosity (cP)_to_UV Power (J s-1): 'values'\n",
      "Warning: Could not generate partial dependence plot for Core Formulation_64/24/10/2 HPC/PDO/CaCl2/PEG550DMA: 'values'\n",
      "Warning: Could not generate partial dependence plot for Dispersed Flow Rate (mL/min)_to_Vessel size (mL): 'values'\n",
      "\n",
      "Tuning ElasticNet...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "ElasticNet - Best parameters: {'model__alpha': 0.6024145688620425, 'model__l1_ratio': 0.046450412719997725}\n",
      "ElasticNet - Best CV score: 0.3093 (RMSE)\n",
      "ElasticNet - Test RMSE: 0.3247\n",
      "ElasticNet - Test R2: 0.0533\n",
      "ElasticNet - Test MAE: 0.2906\n",
      "\n",
      "Comparing model performance...\n",
      "\n",
      "Model Comparison:\n",
      "                      Model      RMSE        R2       MAE   CV_RMSE\n",
      "0      RandomForest (Tuned)  0.191634  0.670302  0.143143  0.205070\n",
      "1  GradientBoosting (Tuned)  0.203286  0.628992  0.150375  0.195852\n",
      "2           XGBoost (Tuned)  0.210515  0.602136  0.161021  0.198702\n",
      "3        ElasticNet (Tuned)  0.324721  0.053349  0.290574  0.309263\n",
      "\n",
      "Best model: RandomForest (Tuned)\n",
      "\n",
      "Generating feature effect report...\n",
      "\n",
      "Prediction Function Created:\n",
      "Example usage:\n",
      "predict_dry_percentage({\n",
      "    'Core Viscosity (cP)': value,\n",
      "    'UV viscosity (cP)': value,\n",
      "    'Emulsion viscosity (cP)': value,\n",
      "    'Core Formulation': 'value',\n",
      "    'Impellor': 'value',\n",
      "    ...\n",
      "})\n",
      "\n",
      "Saving model files...\n",
      "Model files saved to the 'Predictions-Final/models' folder\n",
      "- best_model.pkl: The trained model\n",
      "- model_features.json: Feature information\n",
      "- predict.py: Command-line prediction script\n",
      "- prediction_gui.py: Simple GUI application for predictions\n",
      "\n",
      "Report generated: ml_analysis_report.md\n",
      "\n",
      "Analysis complete!\n",
      "Total execution time: 0h 1m 56s\n",
      "Best model: RandomForest (Tuned)\n",
      "All results saved to 'Predictions-Final' folder\n",
      "Please check the generated report and figures for detailed analysis.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression, VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy.stats import randint, uniform, boxcox\n",
    "import xgboost as XGBRegressor\n",
    "from sklearn.inspection import permutation_importance, partial_dependence\n",
    "import warnings\n",
    "import joblib\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up matplotlib style for professional visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "colors = sns.color_palette(\"viridis\", 8)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"Predictions-Final\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)  # Remove if exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create subdirectories\n",
    "os.makedirs(os.path.join(output_dir, \"figures\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"data\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"models\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"results\"), exist_ok=True)\n",
    "\n",
    "# File path\n",
    "file_path = r\"C:\\Users\\Dani\\AXF-1 data update 29APR25.xlsx\"\n",
    "\n",
    "def save_figure(fig, filename, dpi=300):\n",
    "    \"\"\"Safely save a figure with proper path handling\"\"\"\n",
    "    # Replace problematic characters in filename\n",
    "    safe_filename = re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "    # Save to the output directory\n",
    "    fig_path = os.path.join(output_dir, \"figures\", safe_filename)\n",
    "    fig.savefig(fig_path, dpi=dpi, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    return fig_path\n",
    "\n",
    "def load_and_explore_data(file_path):\n",
    "    \"\"\"Load and explore the dataset\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nColumn information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    \n",
    "    print(\"\\nMissing values by column:\")\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Values': missing_values,\n",
    "        'Percentage': missing_percentage\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing Values'] > 0].sort_values('Percentage', ascending=False))\n",
    "    \n",
    "    # Save missing values table\n",
    "    missing_df.to_csv(os.path.join(output_dir, 'data', 'missing_values.csv'))\n",
    "    \n",
    "    # Check target variable\n",
    "    target = '% dry after 24h in incubator'\n",
    "    if target in df.columns:\n",
    "        print(f\"\\nTarget variable '{target}' statistics:\")\n",
    "        print(df[target].describe())\n",
    "        \n",
    "        # Distribution plot for target\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        ax = sns.histplot(df[target].dropna(), kde=True, bins=20, color=colors[0])\n",
    "        ax.set_title(f'Distribution of {target}', fontsize=15)\n",
    "        ax.set_xlabel(target, fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), 'target_distribution.png')\n",
    "        \n",
    "        # Check for skewness\n",
    "        skewness = df[target].skew()\n",
    "        print(f\"Target skewness: {skewness}\")\n",
    "        \n",
    "        if abs(skewness) > 1:\n",
    "            print(\"Target is highly skewed, consider transformation\")\n",
    "            # Try log transformation\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            ax = sns.histplot(np.log1p(df[target].dropna()), kde=True, bins=20, color=colors[1])\n",
    "            ax.set_title(f'Log-transformed Distribution of {target}', fontsize=15)\n",
    "            ax.set_xlabel(f'Log(1+{target})', fontsize=12)\n",
    "            ax.set_ylabel('Frequency', fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            save_figure(plt.gcf(), 'target_log_distribution.png')\n",
    "    else:\n",
    "        print(f\"\\nTarget variable '{target}' not found in columns.\")\n",
    "        # Look for similar column names\n",
    "        similar_cols = [col for col in df.columns if 'dry' in col.lower() or '24h' in col.lower()]\n",
    "        if similar_cols:\n",
    "            print(f\"Found similar columns: {similar_cols}\")\n",
    "            target = similar_cols[0]  # Use the first similar column as target\n",
    "            print(f\"Using '{target}' as target variable\")\n",
    "    \n",
    "    return df, target\n",
    "\n",
    "def identify_relevant_columns(df, target):\n",
    "    \"\"\"Identify columns that are relevant for prediction and exclude metadata and outcome columns\"\"\"\n",
    "    # Define outcome columns that should not be used for prediction\n",
    "    outcome_columns = [\n",
    "        '% dry after 24h in incubator',        # Target variable\n",
    "        '% dry after 2 days 70% humidity',      # Related outcome \n",
    "        'Droplet/particle size (µm)',           # Outcome measurement\n",
    "        'Droplet/particle size range',          # Outcome measurement\n",
    "        'Droplet/particle size range STDEV',    # Outcome measurement\n",
    "        '% single core',                        # Outcome measurement\n",
    "        '% empty',                              # Outcome measurement\n",
    "        'Polydispersity Index',                 # Outcome measurement\n",
    "        'Transmission window'                   # Outcome measurement\n",
    "    ]\n",
    "    \n",
    "    # Columns to exclude (metadata, identifiers, experimental notes)\n",
    "    metadata_columns = [\n",
    "        'Column 1',                # Sample ID\n",
    "        'Date of experiment',      # Timestamp\n",
    "        'Aims & Hypothesis',       # Experimental context\n",
    "        'Notes'                    # Free text notes\n",
    "    ]\n",
    "    \n",
    "    # Remove UV viscosity (cP) due to multicollinearity with Emulsion viscosity\n",
    "    multicollinearity_columns = [\n",
    "        'UV viscosity (cP)'\n",
    "    ]\n",
    "    \n",
    "    # Columns with too many missing values\n",
    "    high_missing_columns = [\n",
    "        'Temperature of Outer Phase'  # >50% missing values\n",
    "    ]\n",
    "    \n",
    "    # Get all columns\n",
    "    all_cols = df.columns.tolist()\n",
    "    \n",
    "    # Remove excluded columns\n",
    "    excluded_columns = outcome_columns + metadata_columns + multicollinearity_columns + high_missing_columns\n",
    "    relevant_cols = [col for col in all_cols if col not in excluded_columns]\n",
    "    \n",
    "    print(f\"\\nExcluding metadata columns: {metadata_columns}\")\n",
    "    print(f\"Excluding outcome columns: {outcome_columns}\")\n",
    "    print(f\"Excluding multicollinear columns: {multicollinearity_columns}\")\n",
    "    print(f\"Excluding high missing value columns: {high_missing_columns}\")\n",
    "    print(f\"Using {len(relevant_cols)} columns for analysis:\")\n",
    "    print(relevant_cols)\n",
    "    \n",
    "    # Save the list of relevant columns\n",
    "    with open(os.path.join(output_dir, 'data', 'relevant_columns.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(relevant_cols))\n",
    "    \n",
    "    return relevant_cols\n",
    "\n",
    "def analyze_missing_values(df, relevant_cols):\n",
    "    \"\"\"Analyze missing value patterns to inform imputation strategy\"\"\"\n",
    "    print(\"\\nAnalyzing missing value patterns...\")\n",
    "    \n",
    "    # Missing value correlation\n",
    "    missing_matrix = df[relevant_cols].isnull().corr()\n",
    "    \n",
    "    # Plot heatmap of missing value correlation\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    mask = np.triu(np.ones_like(missing_matrix))\n",
    "    ax = sns.heatmap(missing_matrix, mask=mask, annot=False, cmap='coolwarm', \n",
    "                    vmin=-1, vmax=1, center=0)\n",
    "    ax.set_title('Missing Value Correlation Matrix', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    save_figure(plt.gcf(), 'missing_value_correlation.png')\n",
    "    \n",
    "    # Group columns by missing percentage\n",
    "    missing_percentage = df[relevant_cols].isnull().mean() * 100\n",
    "    \n",
    "    # Save missing percentage for relevant columns\n",
    "    missing_percentage.to_csv(os.path.join(output_dir, 'data', 'relevant_columns_missing_percentage.csv'))\n",
    "    \n",
    "    # Bar plot of missing percentages\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    missing_sorted = missing_percentage.sort_values(ascending=False)\n",
    "    ax = sns.barplot(x=missing_sorted.index, y=missing_sorted.values, palette='viridis')\n",
    "    ax.set_title('Missing Value Percentages by Column', fontsize=16)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.set_ylabel('Missing Percentage (%)', fontsize=14)\n",
    "    ax.axhline(y=50, color='red', linestyle='--', label='50% threshold')\n",
    "    ax.axhline(y=30, color='orange', linestyle='--', label='30% threshold')\n",
    "    ax.axhline(y=10, color='green', linestyle='--', label='10% threshold')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    save_figure(plt.gcf(), 'missing_percentages.png')\n",
    "    \n",
    "    # Categories of missing data\n",
    "    low_missing = missing_percentage[missing_percentage < 10].index.tolist()\n",
    "    medium_missing = missing_percentage[(missing_percentage >= 10) & (missing_percentage < 30)].index.tolist()\n",
    "    high_missing = missing_percentage[(missing_percentage >= 30) & (missing_percentage < 50)].index.tolist()\n",
    "    very_high_missing = missing_percentage[missing_percentage >= 50].index.tolist()\n",
    "    \n",
    "    print(f\"\\nColumns with low missing percentage (<10%): {len(low_missing)}\")\n",
    "    for col in low_missing:\n",
    "        print(f\"  - {col}: {missing_percentage[col]:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nColumns with medium missing percentage (10-30%): {len(medium_missing)}\")\n",
    "    for col in medium_missing:\n",
    "        print(f\"  - {col}: {missing_percentage[col]:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nColumns with high missing percentage (30-50%): {len(high_missing)}\")\n",
    "    for col in high_missing:\n",
    "        print(f\"  - {col}: {missing_percentage[col]:.2f}%\")\n",
    "    \n",
    "    # Check if any columns should be dropped due to too many missing values\n",
    "    if very_high_missing:\n",
    "        print(f\"\\nColumns with >50% missing values (consider dropping): {len(very_high_missing)}\")\n",
    "        for col in very_high_missing:\n",
    "            print(f\"  - {col}: {missing_percentage[col]:.2f}%\")\n",
    "    \n",
    "    # Save missing value analysis to file\n",
    "    with open(os.path.join(output_dir, 'results', 'missing_value_analysis.txt'), 'w') as f:\n",
    "        f.write(\"Missing Value Analysis\\n\")\n",
    "        f.write(\"=====================\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Columns with low missing percentage (<10%): {len(low_missing)}\\n\")\n",
    "        for col in low_missing:\n",
    "            f.write(f\"  - {col}: {missing_percentage[col]:.2f}%\\n\")\n",
    "        \n",
    "        f.write(f\"\\nColumns with medium missing percentage (10-30%): {len(medium_missing)}\\n\")\n",
    "        for col in medium_missing:\n",
    "            f.write(f\"  - {col}: {missing_percentage[col]:.2f}%\\n\")\n",
    "        \n",
    "        f.write(f\"\\nColumns with high missing percentage (30-50%): {len(high_missing)}\\n\")\n",
    "        for col in high_missing:\n",
    "            f.write(f\"  - {col}: {missing_percentage[col]:.2f}%\\n\")\n",
    "        \n",
    "        if very_high_missing:\n",
    "            f.write(f\"\\nColumns with >50% missing values (consider dropping): {len(very_high_missing)}\\n\")\n",
    "            for col in very_high_missing:\n",
    "                f.write(f\"  - {col}: {missing_percentage[col]:.2f}%\\n\")\n",
    "    \n",
    "    return {\n",
    "        'low_missing': low_missing,\n",
    "        'medium_missing': medium_missing,\n",
    "        'high_missing': high_missing,\n",
    "        'very_high_missing': very_high_missing\n",
    "    }\n",
    "\n",
    "def analyze_categorical_features(df, target, relevant_cols):\n",
    "    \"\"\"Analyze categorical features and their relationship with target\"\"\"\n",
    "    print(\"\\nAnalyzing categorical features...\")\n",
    "    \n",
    "    # Identify categorical columns from relevant columns\n",
    "    categorical_cols = df[relevant_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "    print(f\"Relevant categorical columns: {categorical_cols}\")\n",
    "    \n",
    "    # Create summary dataframe for categorical columns\n",
    "    cat_summary = []\n",
    "    \n",
    "    # For each categorical column, check distribution and relationship with target\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\nAnalyzing {col}:\")\n",
    "        # Distribution\n",
    "        value_counts = df[col].value_counts()\n",
    "        unique_count = len(value_counts)\n",
    "        top_value = value_counts.index[0]\n",
    "        top_value_percent = (value_counts.iloc[0] / value_counts.sum()) * 100\n",
    "        \n",
    "        print(f\"Number of unique values: {unique_count}\")\n",
    "        print(\"Top 5 most common values:\")\n",
    "        print(value_counts.head())\n",
    "        \n",
    "        # Save value counts\n",
    "        value_counts.to_csv(os.path.join(output_dir, 'data', f'{col}_value_counts.csv'))\n",
    "        \n",
    "        # Add to summary\n",
    "        cat_summary.append({\n",
    "            'Column': col,\n",
    "            'Unique_Values': unique_count,\n",
    "            'Top_Value': top_value,\n",
    "            'Top_Value_Percent': top_value_percent,\n",
    "            'Missing_Percent': df[col].isnull().mean() * 100\n",
    "        })\n",
    "        \n",
    "        # Check if too many unique values (potential issue for one-hot encoding)\n",
    "        if unique_count > 10:\n",
    "            print(f\"WARNING: Column {col} has {unique_count} unique values. May cause overfitting with one-hot encoding.\")\n",
    "        \n",
    "        # Plot value distribution\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        ax = value_counts.nlargest(10).plot(kind='bar', color=colors[2])\n",
    "        ax.set_title(f'Top 10 values in {col}', fontsize=15)\n",
    "        ax.set_xlabel(col, fontsize=12)\n",
    "        ax.set_ylabel('Count', fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), f'{col}_distribution.png')\n",
    "        \n",
    "        # Box plot of target by categorical variable (top 5 categories)\n",
    "        if target in df.columns:\n",
    "            plt.figure(figsize=(16, 10))\n",
    "            top_categories = value_counts.nlargest(5).index\n",
    "            filtered_data = df[df[col].isin(top_categories)]\n",
    "            \n",
    "            if not filtered_data.empty and not filtered_data[target].dropna().empty:\n",
    "                ax = sns.boxplot(x=col, y=target, data=filtered_data, palette='viridis')\n",
    "                ax.set_title(f'Relationship between {col} and {target}', fontsize=15)\n",
    "                ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "                ax.set_xlabel(col, fontsize=12)\n",
    "                ax.set_ylabel(target, fontsize=12)\n",
    "                plt.tight_layout()\n",
    "                save_figure(plt.gcf(), f'{col}_target_relationship.png')\n",
    "                \n",
    "                # Calculate ANOVA to test if categories are significantly different\n",
    "                categories = filtered_data[col].unique()\n",
    "                if len(categories) > 1:\n",
    "                    anova_data = []\n",
    "                    for category in categories:\n",
    "                        cat_values = filtered_data[filtered_data[col] == category][target].dropna()\n",
    "                        if len(cat_values) > 0:\n",
    "                            anova_data.append(cat_values)\n",
    "                    \n",
    "                    if len(anova_data) > 1 and all(len(x) > 0 for x in anova_data):\n",
    "                        try:\n",
    "                            f_stat, p_value = stats.f_oneway(*anova_data)\n",
    "                            print(f\"ANOVA for {col}: F-statistic = {f_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "                            \n",
    "                            # Add to summary\n",
    "                            cat_summary[-1]['ANOVA_p_value'] = p_value\n",
    "                            cat_summary[-1]['Significant'] = p_value < 0.05\n",
    "                        except:\n",
    "                            print(f\"Could not perform ANOVA for {col}\")\n",
    "    \n",
    "    # Save categorical summary\n",
    "    pd.DataFrame(cat_summary).to_csv(os.path.join(output_dir, 'results', 'categorical_summary.csv'), index=False)\n",
    "    \n",
    "    return categorical_cols\n",
    "\n",
    "def analyze_numerical_features(df, target, relevant_cols):\n",
    "    \"\"\"Analyze numerical features and their relationship with target\"\"\"\n",
    "    print(\"\\nAnalyzing numerical features...\")\n",
    "    \n",
    "    # Identify numerical columns from relevant columns\n",
    "    numerical_cols = df[relevant_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    print(f\"Relevant numerical columns: {numerical_cols}\")\n",
    "    \n",
    "    # Create summary dataframe for numerical columns\n",
    "    num_summary = []\n",
    "    \n",
    "    # Analyze distributions\n",
    "    for col in numerical_cols:\n",
    "        # Get statistics\n",
    "        stats_dict = {\n",
    "            'Column': col,\n",
    "            'Mean': df[col].mean(),\n",
    "            'Median': df[col].median(),\n",
    "            'Std': df[col].std(),\n",
    "            'Min': df[col].min(),\n",
    "            'Max': df[col].max(),\n",
    "            'Missing_Percent': df[col].isnull().mean() * 100,\n",
    "            'Skewness': df[col].skew(),\n",
    "            'Kurtosis': df[col].kurtosis()\n",
    "        }\n",
    "        \n",
    "        # Plot distribution and qq-plot\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # Histogram with KDE\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[col].dropna(), kde=True, color=colors[3])\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel(col)\n",
    "        \n",
    "        # QQ plot\n",
    "        plt.subplot(1, 2, 2)\n",
    "        stats.probplot(df[col].dropna(), dist=\"norm\", plot=plt)\n",
    "        plt.title(f'Q-Q Plot of {col} (Skewness: {stats_dict[\"Skewness\"]:.2f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), f'{col}_distribution_qq.png')\n",
    "        \n",
    "        num_summary.append(stats_dict)\n",
    "    \n",
    "    # Save numerical summary\n",
    "    pd.DataFrame(num_summary).to_csv(os.path.join(output_dir, 'results', 'numerical_summary.csv'), index=False)\n",
    "    \n",
    "    # Compute correlation with target\n",
    "    if target in df.columns:\n",
    "        # Filter for rows where target is not null\n",
    "        df_corr = df[numerical_cols + [target]].dropna(subset=[target])\n",
    "        correlations = df_corr.corr()[target].sort_values(ascending=False)\n",
    "        print(\"\\nCorrelations with target variable:\")\n",
    "        print(correlations)\n",
    "        \n",
    "        # Save correlations\n",
    "        correlations.to_csv(os.path.join(output_dir, 'results', 'target_correlations.csv'))\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        correlation_matrix = df_corr.corr()\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        ax = sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                    square=True, linewidths=.5)\n",
    "        ax.set_title('Correlation Matrix', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), 'correlation_heatmap.png')\n",
    "        \n",
    "        # Plot correlations with target as bar chart\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        correlations_without_target = correlations.drop(target)\n",
    "        ax = correlations_without_target.sort_values().plot(kind='barh', color=[colors[i % len(colors)] for i in range(len(correlations_without_target))])\n",
    "        ax.set_title(f'Correlation with {target}', fontsize=15)\n",
    "        ax.set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "        plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), 'target_correlation_barchart.png')\n",
    "        \n",
    "        # Plot scatter plots for top correlated features\n",
    "        top_correlated = correlations.drop(target).abs().nlargest(5).index.tolist()\n",
    "        for col in top_correlated:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            ax = sns.scatterplot(x=col, y=target, data=df, alpha=0.6, color=colors[4])\n",
    "            ax.set_title(f'Scatter plot of {target} vs {col}', fontsize=15)\n",
    "            ax.set_xlabel(col, fontsize=12)\n",
    "            ax.set_ylabel(target, fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            save_figure(plt.gcf(), f'{col}_scatter.png')\n",
    "            \n",
    "            # Add regression plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            ax = sns.regplot(x=col, y=target, data=df, scatter_kws={'alpha':0.5}, line_kws={'color':colors[5]})\n",
    "            ax.set_title(f'Regression plot of {target} vs {col}', fontsize=15)\n",
    "            ax.set_xlabel(col, fontsize=12)\n",
    "            ax.set_ylabel(target, fontsize=12)\n",
    "            \n",
    "            # Add correlation coefficient in the plot\n",
    "            corr_val = df[[col, target]].corr().iloc[0, 1]\n",
    "            ax.annotate(f'r = {corr_val:.3f}', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                       fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            save_figure(plt.gcf(), f'{col}_regplot.png')\n",
    "    \n",
    "    # Check for collinearity among numerical features\n",
    "    correlation_matrix_num = df[numerical_cols].corr()\n",
    "    \n",
    "    # Find highly correlated features\n",
    "    high_corr_threshold = 0.8\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(correlation_matrix_num.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix_num.columns)):\n",
    "            if abs(correlation_matrix_num.iloc[i, j]) > high_corr_threshold:\n",
    "                high_corr_pairs.append(\n",
    "                    (correlation_matrix_num.columns[i], \n",
    "                     correlation_matrix_num.columns[j], \n",
    "                     correlation_matrix_num.iloc[i, j])\n",
    "                )\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(\"\\nHighly correlated feature pairs (correlation > 0.8):\")\n",
    "        for col1, col2, corr in high_corr_pairs:\n",
    "            print(f\"{col1} and {col2}: {corr:.3f}\")\n",
    "            \n",
    "        # Save highly correlated pairs\n",
    "        pd.DataFrame(high_corr_pairs, columns=['Feature 1', 'Feature 2', 'Correlation']) \\\n",
    "            .to_csv(os.path.join(output_dir, 'results', 'high_correlation_pairs.csv'), index=False)\n",
    "        \n",
    "        # Plot correlation network for highly correlated pairs\n",
    "        if len(high_corr_pairs) > 0:\n",
    "            try:\n",
    "                import networkx as nx\n",
    "                \n",
    "                plt.figure(figsize=(12, 10))\n",
    "                G = nx.Graph()\n",
    "                \n",
    "                # Add nodes\n",
    "                for col in numerical_cols:\n",
    "                    G.add_node(col)\n",
    "                \n",
    "                # Add edges with weights\n",
    "                for col1, col2, corr in high_corr_pairs:\n",
    "                    G.add_edge(col1, col2, weight=abs(corr))\n",
    "                \n",
    "                # Set positions\n",
    "                pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "                \n",
    "                # Draw\n",
    "                nx.draw_networkx_nodes(G, pos, node_size=700, node_color=colors[6], alpha=0.8)\n",
    "                nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "                \n",
    "                # Draw edges with width proportional to correlation\n",
    "                for u, v, d in G.edges(data=True):\n",
    "                    width = d['weight'] * 5\n",
    "                    nx.draw_networkx_edges(G, pos, edgelist=[(u, v)], width=width, alpha=0.7)\n",
    "                \n",
    "                # Add edge labels (correlations)\n",
    "                edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "                edge_labels = {k: f'{v:.2f}' for k, v in edge_labels.items()}\n",
    "                nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "                \n",
    "                plt.title('Correlation Network (correlations > 0.8)', fontsize=15)\n",
    "                plt.axis('off')\n",
    "                plt.tight_layout()\n",
    "                save_figure(plt.gcf(), 'correlation_network.png')\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"NetworkX library not available. Skipping correlation network plot.\")\n",
    "    \n",
    "    return numerical_cols\n",
    "\n",
    "def filter_categorical_columns(df, categorical_cols, threshold=10):\n",
    "    \"\"\"Filter categorical columns based on cardinality to avoid overfitting\"\"\"\n",
    "    filtered_categorical_cols = []\n",
    "    high_cardinality_cols = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        n_unique = df[col].nunique()\n",
    "        if n_unique <= threshold:\n",
    "            filtered_categorical_cols.append(col)\n",
    "        else:\n",
    "            high_cardinality_cols.append(col)\n",
    "    \n",
    "    if high_cardinality_cols:\n",
    "        print(f\"\\nExcluding high-cardinality categorical columns to avoid overfitting: {high_cardinality_cols}\")\n",
    "    \n",
    "    print(f\"Using {len(filtered_categorical_cols)} categorical columns for modeling\")\n",
    "    \n",
    "    # Save the list of filtered categorical columns\n",
    "    with open(os.path.join(output_dir, 'data', 'filtered_categorical_columns.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(filtered_categorical_cols))\n",
    "    \n",
    "    return filtered_categorical_cols\n",
    "\n",
    "def feature_engineering(df, numerical_cols, categorical_cols, target):\n",
    "    \"\"\"Create new features using various methods with physical meaning\"\"\"\n",
    "    print(\"\\nPerforming feature engineering...\")\n",
    "    \n",
    "    # Define outcome columns that should not be used for prediction\n",
    "    outcome_columns = [\n",
    "        '% dry after 24h in incubator',        # Target variable\n",
    "        '% dry after 2 days 70% humidity',      # Related outcome \n",
    "        'Droplet/particle size (µm)',           # Outcome measurement\n",
    "        'Droplet/particle size range',          # Outcome measurement\n",
    "        'Droplet/particle size range STDEV',    # Outcome measurement\n",
    "        '% single core',                        # Outcome measurement\n",
    "        '% empty',                              # Outcome measurement\n",
    "        'Polydispersity Index',                 # Outcome measurement\n",
    "        'Transmission window'                   # Outcome measurement\n",
    "    ]\n",
    "    \n",
    "    df_engineered = df.copy()\n",
    "    created_features = []\n",
    "    \n",
    "    # 1. INTERACTION FEATURES\n",
    "    # Identify numerical columns with highest correlation to target\n",
    "    if target in df.columns:\n",
    "        target_data = df_engineered[[target] + numerical_cols].dropna()\n",
    "        correlations = target_data.corr()[target].abs().sort_values(ascending=False)\n",
    "        top_features = correlations.drop(target).head(5).index.tolist()\n",
    "        \n",
    "        print(f\"\\nCreating interaction features for top correlated features: {top_features}\")\n",
    "        \n",
    "        # Create interactions between top features\n",
    "        for i in range(len(top_features)):\n",
    "            for j in range(i + 1, len(top_features)):\n",
    "                col1 = top_features[i]\n",
    "                col2 = top_features[j]\n",
    "                \n",
    "                # Multiplication interaction\n",
    "                interaction_name = f\"{col1}_x_{col2}\"\n",
    "                df_engineered[interaction_name] = df_engineered[col1] * df_engineered[col2]\n",
    "                created_features.append({'name': interaction_name, 'type': 'interaction', 'formula': f\"{col1} * {col2}\"})\n",
    "                \n",
    "                # Ratio interaction (if it makes physical sense)\n",
    "                ratio_name = f\"{col1}_to_{col2}\"\n",
    "                # Avoid division by zero\n",
    "                df_engineered[ratio_name] = df_engineered[col1] / df_engineered[col2].replace(0, np.nan)\n",
    "                created_features.append({'name': ratio_name, 'type': 'ratio', 'formula': f\"{col1} / {col2}\"})\n",
    "        \n",
    "        # Create physically meaningful combinations\n",
    "        # Flow-related ratios\n",
    "        if 'Dispersed Flow Rate (mL/min)' in numerical_cols and 'Continuous Flow Rate (mL/min)' in numerical_cols:\n",
    "            # Flow ratio\n",
    "            df_engineered['Flow_Ratio'] = df_engineered['Dispersed Flow Rate (mL/min)'] / df_engineered['Continuous Flow Rate (mL/min)'].replace(0, np.nan)\n",
    "            created_features.append({'name': 'Flow_Ratio', 'type': 'physical_ratio', 'formula': 'Dispersed Flow Rate / Continuous Flow Rate'})\n",
    "            \n",
    "        # Energy density\n",
    "        if 'UV Power (J s-1)' in numerical_cols and 'Dispersed Flow Rate (mL/min)' in numerical_cols:\n",
    "            # Energy per volume\n",
    "            df_engineered['Energy_Density'] = df_engineered['UV Power (J s-1)'] / df_engineered['Dispersed Flow Rate (mL/min)'].replace(0, np.nan)\n",
    "            created_features.append({'name': 'Energy_Density', 'type': 'physical_ratio', 'formula': 'UV Power / Dispersed Flow Rate'})\n",
    "        \n",
    "        # Residence time approximation\n",
    "        if 'Length of curing tubing (m)' in numerical_cols and 'Continuous Flow Rate (mL/min)' in numerical_cols:\n",
    "            # Residence time = length / flow rate\n",
    "            df_engineered['Residence_Time'] = df_engineered['Length of curing tubing (m)'] / df_engineered['Continuous Flow Rate (mL/min)'].replace(0, np.nan)\n",
    "            created_features.append({'name': 'Residence_Time', 'type': 'physical_ratio', 'formula': 'Length of curing tubing / Continuous Flow Rate'})\n",
    "        \n",
    "        # Viscosity-related features\n",
    "        if 'Emulsion viscosity (cP)' in numerical_cols and 'Core Viscosity (cP)' in numerical_cols:\n",
    "            # Viscosity ratio\n",
    "            df_engineered['Viscosity_Ratio'] = df_engineered['Emulsion viscosity (cP)'] / df_engineered['Core Viscosity (cP)'].replace(0, np.nan)\n",
    "            created_features.append({'name': 'Viscosity_Ratio', 'type': 'physical_ratio', 'formula': 'Emulsion viscosity / Core Viscosity'})\n",
    "        \n",
    "        print(f\"Created {len(created_features)} interaction and physical features\")\n",
    "    \n",
    "    # 2. POLYNOMIAL FEATURES\n",
    "    # Create polynomial features for top numerical features\n",
    "    poly_features = []\n",
    "    if len(top_features) > 0:\n",
    "        for feature in top_features[:3]:  # Limit to top 3 to avoid explosion of features\n",
    "            # Square\n",
    "            square_name = f\"{feature}_squared\"\n",
    "            df_engineered[square_name] = df_engineered[feature] ** 2\n",
    "            poly_features.append({'name': square_name, 'type': 'polynomial', 'formula': f\"{feature}^2\"})\n",
    "            \n",
    "            # Square root (for positive features)\n",
    "            if df_engineered[feature].dropna().min() >= 0:\n",
    "                sqrt_name = f\"{feature}_sqrt\"\n",
    "                df_engineered[sqrt_name] = np.sqrt(df_engineered[feature])\n",
    "                poly_features.append({'name': sqrt_name, 'type': 'polynomial', 'formula': f\"sqrt({feature})\"})\n",
    "        \n",
    "        created_features.extend(poly_features)\n",
    "        print(f\"Created {len(poly_features)} polynomial features\")\n",
    "    \n",
    "    # 3. BINNING\n",
    "    # Create bins for selected numerical features\n",
    "    binned_features = []\n",
    "    for feature in top_features[:2]:  # Limit to top 2\n",
    "        binned_name = f\"{feature}_binned\"\n",
    "        # Create 5 bins based on quantiles\n",
    "        df_engineered[binned_name] = pd.qcut(df_engineered[feature], q=5, labels=False, duplicates='drop')\n",
    "        binned_features.append({'name': binned_name, 'type': 'binned', 'formula': f\"qcut({feature}, 5)\"})\n",
    "    \n",
    "    created_features.extend(binned_features)\n",
    "    print(f\"Created {len(binned_features)} binned features\")\n",
    "    \n",
    "    # 4. TRANSFORMATIONS\n",
    "    # Apply transformations to skewed features\n",
    "    transformed_features = []\n",
    "    for col in numerical_cols:\n",
    "        # Skip columns with negative values or all zeros for log transform\n",
    "        if df_engineered[col].dropna().min() <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Check skewness\n",
    "        skewness = df_engineered[col].skew()\n",
    "        \n",
    "        # Apply transformations to highly skewed features\n",
    "        if abs(skewness) > 1:\n",
    "            # Log transformation\n",
    "            log_name = f\"{col}_log\"\n",
    "            df_engineered[log_name] = np.log1p(df_engineered[col])\n",
    "            transformed_features.append({'name': log_name, 'type': 'log_transform', 'formula': f\"log(1+{col})\"})\n",
    "    \n",
    "    created_features.extend(transformed_features)\n",
    "    print(f\"Created {len(transformed_features)} transformed features\")\n",
    "    \n",
    "    # Save created features information\n",
    "    pd.DataFrame(created_features).to_csv(os.path.join(output_dir, 'results', 'engineered_features.csv'), index=False)\n",
    "    \n",
    "    # Visualize distribution of top engineered features\n",
    "    # Select a few top engineered features for visualization\n",
    "    if len(created_features) > 0:\n",
    "        top_engineered = [f['name'] for f in created_features[:min(6, len(created_features))]]\n",
    "        for col in top_engineered:\n",
    "            if col in df_engineered.columns:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                sns.histplot(df_engineered[col].dropna(), kde=True, color=colors[1])\n",
    "                plt.title(f'Distribution of {col}')\n",
    "                plt.tight_layout()\n",
    "                save_figure(plt.gcf(), f'engineered_{col}_distribution.png')\n",
    "    \n",
    "    # Get new lists of feature columns\n",
    "    new_numerical_cols = df_engineered.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    new_numerical_cols = [col for col in new_numerical_cols if col != target and col not in outcome_columns]\n",
    "    \n",
    "    return df_engineered, new_numerical_cols, created_features\n",
    "\n",
    "def handle_outliers(df, numerical_cols, method='iqr', threshold=1.5):\n",
    "    \"\"\"Detect and handle outliers in numerical features\"\"\"\n",
    "    print(\"\\nHandling outliers in numerical features...\")\n",
    "    \n",
    "    # Copy the original dataframe to avoid modifying it\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # For each numerical column, detect and handle outliers\n",
    "    outlier_info = {}\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        # Skip columns with too many missing values\n",
    "        if df_processed[col].isnull().sum() / len(df_processed) > 0.5:\n",
    "            print(f\"Skipping {col} for outlier detection due to high missing percentage\")\n",
    "            continue\n",
    "        \n",
    "        # Detect outliers\n",
    "        if method == 'iqr':\n",
    "            Q1 = df_processed[col].quantile(0.25)\n",
    "            Q3 = df_processed[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            \n",
    "            # Get outliers\n",
    "            outliers = df_processed[(df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)][col]\n",
    "            \n",
    "            outlier_info[col] = {\n",
    "                'method': 'IQR',\n",
    "                'threshold': threshold,\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound,\n",
    "                'outlier_count': len(outliers),\n",
    "                'outlier_percentage': len(outliers) / df_processed[col].count() * 100\n",
    "            }\n",
    "            \n",
    "            # Cap the outliers\n",
    "            print(f\"Capping outliers in {col} using IQR method\")\n",
    "            print(f\"  - Lower bound: {lower_bound:.2f}\")\n",
    "            print(f\"  - Upper bound: {upper_bound:.2f}\")\n",
    "            print(f\"  - Outliers found: {len(outliers)} ({len(outliers) / df_processed[col].count() * 100:.2f}%)\")\n",
    "            \n",
    "            df_processed[col] = df_processed[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        elif method == 'zscore':\n",
    "            z_scores = stats.zscore(df_processed[col].dropna())\n",
    "            abs_z_scores = np.abs(z_scores)\n",
    "            outliers_mask = abs_z_scores > threshold\n",
    "            outliers = df_processed[col].dropna().iloc[outliers_mask]\n",
    "            \n",
    "            outlier_info[col] = {\n",
    "                'method': 'Z-score',\n",
    "                'threshold': threshold,\n",
    "                'outlier_count': len(outliers),\n",
    "                'outlier_percentage': len(outliers) / df_processed[col].count() * 100\n",
    "            }\n",
    "            \n",
    "            # Replace outliers with median\n",
    "            if len(outliers) > 0:\n",
    "                print(f\"Replacing outliers in {col} using Z-score method\")\n",
    "                print(f\"  - Z-score threshold: {threshold}\")\n",
    "                print(f\"  - Outliers found: {len(outliers)} ({len(outliers) / df_processed[col].count() * 100:.2f}%)\")\n",
    "                \n",
    "                # Create a series with outlier indices\n",
    "                outlier_indices = df_processed[col].dropna().iloc[outliers_mask].index\n",
    "                df_processed.loc[outlier_indices, col] = df_processed[col].median()\n",
    "    \n",
    "    # Save outlier information\n",
    "    pd.DataFrame(outlier_info).T.to_csv(os.path.join(output_dir, 'results', 'outlier_handling_info.csv'))\n",
    "    \n",
    "    # Visualize before/after for top columns with most outliers\n",
    "    top_outlier_cols = sorted(\n",
    "        [(col, info['outlier_percentage']) for col, info in outlier_info.items()], \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )[:5]\n",
    "    \n",
    "    if top_outlier_cols:\n",
    "        # Create before/after comparison plots\n",
    "        for col, pct in top_outlier_cols:\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            \n",
    "            # Before (original data)\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(df[col].dropna(), kde=True, color=colors[0])\n",
    "            plt.title(f'Before Outlier Handling: {col}\\n({pct:.1f}% outliers)')\n",
    "            \n",
    "            # After (processed data)\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.histplot(df_processed[col].dropna(), kde=True, color=colors[1])\n",
    "            plt.title(f'After Outlier Handling: {col}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            save_figure(plt.gcf(), f'outlier_handling_{col}.png')\n",
    "    \n",
    "    return df_processed, outlier_info\n",
    "\n",
    "def create_imputation_strategy(df, numerical_cols, categorical_cols, missing_patterns):\n",
    "    \"\"\"Create a comprehensive imputation strategy with indicator variables\"\"\"\n",
    "    print(\"\\nDesigning imputation strategy...\")\n",
    "    \n",
    "    # Strategies for numerical features\n",
    "    numerical_imputers = {}\n",
    "    \n",
    "    # For columns with low missing values - use median\n",
    "    for col in [c for c in numerical_cols if c in missing_patterns['low_missing']]:\n",
    "        numerical_imputers[col] = ('median', SimpleImputer(strategy='median'))\n",
    "    \n",
    "    # For columns with medium missing values - use KNN imputation\n",
    "    medium_missing_num = [c for c in numerical_cols if c in missing_patterns['medium_missing']]\n",
    "    if medium_missing_num:\n",
    "        print(f\"Using KNN imputation for {len(medium_missing_num)} numerical columns with medium missing values\")\n",
    "        for col in medium_missing_num:\n",
    "            numerical_imputers[col] = ('knn', KNNImputer(n_neighbors=5))\n",
    "    \n",
    "    # For columns with high missing - consider dropping or advanced methods\n",
    "    high_missing_num = [c for c in numerical_cols if c in missing_patterns['high_missing']]\n",
    "    if high_missing_num:\n",
    "        print(f\"Columns with high missing values (consider dropping or more advanced imputation): {high_missing_num}\")\n",
    "        # Use KNN with more neighbors for more robust estimation\n",
    "        for col in high_missing_num:\n",
    "            numerical_imputers[col] = ('knn', KNNImputer(n_neighbors=10))\n",
    "    \n",
    "    # For categorical columns - always use most frequent\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    \n",
    "    # Create list of columns to add indicator variables for\n",
    "    indicator_cols = []\n",
    "    indicator_cols.extend(medium_missing_num)\n",
    "    indicator_cols.extend(high_missing_num)\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().mean() > 0.05:  # Add indicator if > 5% missing\n",
    "            indicator_cols.append(col)\n",
    "    \n",
    "    # Add columns with very high missing values but which we'll keep\n",
    "    very_high_missing_keep = [c for c in numerical_cols if c in missing_patterns['very_high_missing']]\n",
    "    indicator_cols.extend(very_high_missing_keep)\n",
    "    \n",
    "    print(\"\\nImputation strategy:\")\n",
    "    print(\"- Simple median imputation for numerical columns with <10% missing values\")\n",
    "    print(\"- KNN imputation for numerical columns with 10-30% missing values\")\n",
    "    print(\"- KNN imputation with more neighbors for columns with >30% missing values\")\n",
    "    print(\"- Most frequent value imputation for categorical columns\")\n",
    "    print(f\"- Missing value indicators will be created for {len(indicator_cols)} columns\")\n",
    "    \n",
    "    # Save imputation strategy\n",
    "    with open(os.path.join(output_dir, 'results', 'imputation_strategy.txt'), 'w') as f:\n",
    "        f.write(\"Imputation Strategy:\\n\\n\")\n",
    "        f.write(\"Numerical columns with low missing (<10%):\\n\")\n",
    "        f.write(\"- Method: Median imputation\\n\")\n",
    "        f.write(\"- Columns: \" + ', '.join([c for c in numerical_cols if c in missing_patterns['low_missing']]) + \"\\n\\n\")\n",
    "        \n",
    "        if medium_missing_num:\n",
    "            f.write(\"Numerical columns with medium missing (10-30%):\\n\")\n",
    "            f.write(\"- Method: KNN imputation (n_neighbors=5)\\n\")\n",
    "            f.write(\"- Columns: \" + ', '.join(medium_missing_num) + \"\\n\\n\")\n",
    "        \n",
    "        if high_missing_num:\n",
    "            f.write(\"Numerical columns with high missing (30-50%):\\n\")\n",
    "            f.write(\"- Method: KNN imputation (n_neighbors=10)\\n\")\n",
    "            f.write(\"- Columns: \" + ', '.join(high_missing_num) + \"\\n\\n\")\n",
    "        \n",
    "        if very_high_missing_keep:\n",
    "            f.write(\"Numerical columns with very high missing (>50%) that will be kept:\\n\")\n",
    "            f.write(\"- Method: KNN imputation (n_neighbors=10) + missing indicator\\n\")\n",
    "            f.write(\"- Columns: \" + ', '.join(very_high_missing_keep) + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Categorical columns:\\n\")\n",
    "        f.write(\"- Method: Most frequent value imputation\\n\")\n",
    "        f.write(\"- Columns: \" + ', '.join(categorical_cols) + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Missing value indicators will be created for:\\n\")\n",
    "        f.write(\"- Columns: \" + ', '.join(indicator_cols))\n",
    "    \n",
    "    return numerical_imputers, categorical_imputer, indicator_cols\n",
    "\n",
    "def prepare_data(df, target, categorical_cols, numerical_cols, numerical_imputers, \n",
    "                categorical_imputer, indicator_cols, outlier_handling=True):\n",
    "    \"\"\"Prepare data for modeling with comprehensive preprocessing\"\"\"\n",
    "    print(\"\\nPreparing data for modeling...\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Separate features and target\n",
    "    if target in df.columns:\n",
    "        X = df_processed[categorical_cols + numerical_cols].copy()\n",
    "        y = df_processed[target].copy()\n",
    "    else:\n",
    "        raise ValueError(f\"Target column '{target}' not found in dataframe\")\n",
    "    \n",
    "    # Handle missing values in target\n",
    "    mask = ~y.isna()\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    print(f\"Data after removing rows with missing target: {X.shape}\")\n",
    "    \n",
    "    # Handle outliers if requested\n",
    "    if outlier_handling:\n",
    "        X, outlier_info = handle_outliers(X, numerical_cols)\n",
    "        # Save outlier information\n",
    "        pd.DataFrame(outlier_info).T.to_csv(os.path.join(output_dir, 'results', 'outlier_handling.csv'))\n",
    "    \n",
    "    # Create missing value indicator variables\n",
    "    for col in indicator_cols:\n",
    "        if col in X.columns:\n",
    "            indicator_name = f\"{col}_missing\"\n",
    "            X[indicator_name] = X[col].isnull().astype(int)\n",
    "    \n",
    "    # Apply imputation for numerical columns\n",
    "    for col in numerical_cols:\n",
    "        if col in X.columns and X[col].isnull().sum() > 0:\n",
    "            # Get imputation strategy\n",
    "            if col in numerical_imputers:\n",
    "                strategy, imputer = numerical_imputers[col]\n",
    "                print(f\"Imputing {col} using {strategy} strategy\")\n",
    "                # Create a DataFrame with just this column\n",
    "                col_df = X[[col]]\n",
    "                # Impute\n",
    "                imputed_values = imputer.fit_transform(col_df)\n",
    "                # Replace in original DataFrame\n",
    "                X[col] = imputed_values\n",
    "    \n",
    "    # Apply imputation for categorical columns\n",
    "    for col in categorical_cols:\n",
    "        if col in X.columns and X[col].isnull().sum() > 0:\n",
    "            print(f\"Imputing {col} using most frequent strategy\")\n",
    "            # Create a DataFrame with just this column\n",
    "            col_df = X[[col]].astype(str)  # Convert to string to handle non-string objects\n",
    "            # Impute\n",
    "            imputed_values = categorical_imputer.fit_transform(col_df)\n",
    "            # Replace in original DataFrame\n",
    "            X[col] = imputed_values\n",
    "    \n",
    "    # Check if any missing values remain\n",
    "    missing_after = X.isnull().sum().sum()\n",
    "    if missing_after > 0:\n",
    "        print(f\"WARNING: {missing_after} missing values remain after imputation\")\n",
    "        # List columns with remaining missing values\n",
    "        missing_cols = X.columns[X.isnull().sum() > 0].tolist()\n",
    "        print(f\"Columns with remaining missing values: {missing_cols}\")\n",
    "        \n",
    "        # Fill any remaining missing values with conservative strategies\n",
    "        for col in missing_cols:\n",
    "            if col in numerical_cols:\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "            else:\n",
    "                X[col] = X[col].fillna(X[col].mode()[0])\n",
    "    \n",
    "    # Update numerical columns list to include indicators\n",
    "    numerical_cols_with_indicators = numerical_cols.copy()\n",
    "    for col in indicator_cols:\n",
    "        indicator_name = f\"{col}_missing\"\n",
    "        if indicator_name in X.columns:\n",
    "            numerical_cols_with_indicators.append(indicator_name)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Testing set size: {X_test.shape}\")\n",
    "    \n",
    "    # Save the training and testing sets\n",
    "    X_train.to_csv(os.path.join(output_dir, 'data', 'X_train.csv'), index=False)\n",
    "    X_test.to_csv(os.path.join(output_dir, 'data', 'X_test.csv'), index=False)\n",
    "    y_train.to_csv(os.path.join(output_dir, 'data', 'y_train.csv'), index=False)\n",
    "    y_test.to_csv(os.path.join(output_dir, 'data', 'y_test.csv'), index=False)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, numerical_cols_with_indicators\n",
    "\n",
    "def build_preprocessing_pipeline(numerical_cols, categorical_cols, scaling_method='standard'):\n",
    "    \"\"\"Build comprehensive preprocessing pipeline with multiple options\"\"\"\n",
    "    print(f\"\\nBuilding preprocessing pipeline with {scaling_method} scaling...\")\n",
    "    \n",
    "    transformers = []\n",
    "    \n",
    "    # For numerical features: choose scaling method\n",
    "    if numerical_cols:\n",
    "        if scaling_method == 'standard':\n",
    "            # Standardization (z-score normalization)\n",
    "            transformers.append(('num', StandardScaler(), numerical_cols))\n",
    "        elif scaling_method == 'minmax':\n",
    "            # Min-Max scaling (normalization to [0,1])\n",
    "            transformers.append(('num', MinMaxScaler(), numerical_cols))\n",
    "        elif scaling_method == 'robust':\n",
    "            # Robust scaling (uses quantiles, less affected by outliers)\n",
    "            transformers.append(('num', RobustScaler(), numerical_cols))\n",
    "    \n",
    "    # For categorical features: one-hot encoding\n",
    "    if categorical_cols:\n",
    "        transformers.append(('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols))\n",
    "    \n",
    "    # Combine transformers\n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def select_features(X_train, y_train, numerical_cols, categorical_cols, preprocessor, \n",
    "                   method='importance', n_features=None):\n",
    "    \"\"\"Select the most important features using various methods\"\"\"\n",
    "    print(f\"\\nPerforming feature selection using {method} method...\")\n",
    "    \n",
    "    # Create a pipeline with preprocessing\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "    \n",
    "    # Transform the data\n",
    "    X_train_processed = pipeline.fit_transform(X_train)\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = []\n",
    "    if hasattr(preprocessor, 'transformers_'):\n",
    "        for name, transformer, cols in preprocessor.transformers_:\n",
    "            if name == 'num':\n",
    "                feature_names.extend(cols)\n",
    "            elif name == 'cat':\n",
    "                # For categorical features, get one-hot encoded feature names\n",
    "                ohe = transformer\n",
    "                if hasattr(ohe, 'get_feature_names_out'):\n",
    "                    cat_features = ohe.get_feature_names_out(cols)\n",
    "                    feature_names.extend(cat_features)\n",
    "                else:\n",
    "                    # For categorical features without get_feature_names_out, approximate\n",
    "                    for col in cols:\n",
    "                        unique_values = X_train[col].dropna().unique()\n",
    "                        for val in unique_values:\n",
    "                            feature_names.append(f\"{col}_{val}\")\n",
    "    \n",
    "    # Adjust feature names and processed data to match in length\n",
    "    min_len = min(len(feature_names), X_train_processed.shape[1])\n",
    "    feature_names = feature_names[:min_len]\n",
    "    X_train_processed = X_train_processed[:, :min_len]\n",
    "    \n",
    "    if method == 'importance':\n",
    "        # Use Random Forest for feature importance\n",
    "        if n_features is None:\n",
    "            n_features = min(30, X_train_processed.shape[1])\n",
    "        \n",
    "        selector = SelectFromModel(\n",
    "            RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            max_features=n_features\n",
    "        )\n",
    "        selector.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Get selected feature indices\n",
    "        selected_indices = selector.get_support()\n",
    "        \n",
    "        # Calculate importances\n",
    "        importances = selector.estimator_.feature_importances_\n",
    "        \n",
    "    elif method == 'correlation':\n",
    "        # Use SelectKBest with f_regression for correlation-based selection\n",
    "        if n_features is None:\n",
    "            n_features = min(20, X_train_processed.shape[1])\n",
    "        \n",
    "        selector = SelectKBest(f_regression, k=n_features)\n",
    "        selector.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Get selected feature indices\n",
    "        selected_indices = selector.get_support()\n",
    "        \n",
    "        # Calculate importances\n",
    "        importances = selector.scores_\n",
    "        \n",
    "    elif method == 'stability':\n",
    "        # Stability selection - run multiple feature selections with subsampling\n",
    "        if n_features is None:\n",
    "            n_features = min(30, X_train_processed.shape[1])\n",
    "        \n",
    "        # Initialize counters for feature selection frequency\n",
    "        selection_counts = np.zeros(X_train_processed.shape[1])\n",
    "        n_iterations = 20\n",
    "        \n",
    "        base_selector = SelectFromModel(\n",
    "            RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            max_features=n_features\n",
    "        )\n",
    "        \n",
    "        for i in range(n_iterations):\n",
    "            # Create a random subsample\n",
    "            indices = np.random.choice(\n",
    "                np.arange(X_train_processed.shape[0]),\n",
    "                size=int(0.8 * X_train_processed.shape[0]),\n",
    "                replace=False\n",
    "            )\n",
    "            X_subsample = X_train_processed[indices]\n",
    "            y_subsample = y_train.iloc[indices]\n",
    "            \n",
    "            # Fit on subsample\n",
    "            base_selector.fit(X_subsample, y_subsample)\n",
    "            \n",
    "            # Add to selection counts\n",
    "            selection_counts += base_selector.get_support().astype(int)\n",
    "        \n",
    "        # Select features that were selected in at least 50% of iterations\n",
    "        selected_indices = selection_counts >= (n_iterations / 2)\n",
    "        \n",
    "        # Calculate importances as the fraction of times each feature was selected\n",
    "        importances = selection_counts / n_iterations\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = [feature_names[i] for i in range(len(feature_names)) if selected_indices[i]]\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features out of {len(feature_names)}\")\n",
    "    \n",
    "    # Calculate feature importances for reporting\n",
    "    selected_importances = [importances[i] for i in range(len(importances)) if selected_indices[i]]\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Importance': selected_importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 selected features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Save feature selection results\n",
    "    feature_importance.to_csv(os.path.join(output_dir, 'results', f'selected_features_{method}.csv'), index=False)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    top_features = feature_importance.head(20)\n",
    "    ax = sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
    "    ax.set_title(f'Top 20 Selected Features ({method} method)', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    save_figure(plt.gcf(), f'selected_features_{method}.png')\n",
    "    \n",
    "    return selector, feature_importance, selected_features\n",
    "\n",
    "def hyperparameter_tune(X_train, y_train, X_test, y_test, preprocessor, feature_selector=None):\n",
    "    \"\"\"Hyperparameter tuning for multiple models with feature selection\"\"\"\n",
    "    print(\"\\nPerforming hyperparameter tuning for all models...\")\n",
    "    \n",
    "    # Define the models and their hyperparameter spaces\n",
    "    models = {\n",
    "        'RandomForest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'param_grid': {\n",
    "                'model__n_estimators': randint(100, 500),\n",
    "                'model__max_depth': [None, 10, 20, 30],\n",
    "                'model__min_samples_split': randint(2, 10),\n",
    "                'model__min_samples_leaf': randint(1, 5),\n",
    "                'model__max_features': ['sqrt', 'log2', None]\n",
    "            }\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'model': GradientBoostingRegressor(random_state=42),\n",
    "            'param_grid': {\n",
    "                'model__n_estimators': randint(100, 500),\n",
    "                'model__learning_rate': uniform(0.01, 0.2),\n",
    "                'model__max_depth': randint(3, 10),\n",
    "                'model__min_samples_split': randint(2, 10),\n",
    "                'model__min_samples_leaf': randint(1, 5),\n",
    "                'model__subsample': uniform(0.6, 0.4)\n",
    "            }\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model': XGBRegressor.XGBRegressor(random_state=42),\n",
    "            'param_grid': {\n",
    "                'model__n_estimators': randint(100, 500),\n",
    "                'model__learning_rate': uniform(0.01, 0.2),\n",
    "                'model__max_depth': randint(3, 10),\n",
    "                'model__min_child_weight': randint(1, 10),\n",
    "                'model__subsample': uniform(0.6, 0.4),\n",
    "                'model__colsample_bytree': uniform(0.6, 0.4)\n",
    "            }\n",
    "        },\n",
    "        'ElasticNet': {\n",
    "            'model': ElasticNet(random_state=42),\n",
    "            'param_grid': {\n",
    "                'model__alpha': uniform(0.01, 1),\n",
    "                'model__l1_ratio': uniform(0, 1)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Tune each model and store results\n",
    "    tuned_models = {}\n",
    "    \n",
    "    for name, config in models.items():\n",
    "        print(f\"\\nTuning {name}...\")\n",
    "        \n",
    "        # Create pipeline with preprocessing and optional feature selection\n",
    "        if feature_selector is not None:\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('selector', feature_selector),\n",
    "                ('model', config['model'])\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('model', config['model'])\n",
    "            ])\n",
    "        \n",
    "        # RandomizedSearchCV for efficient tuning\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            param_distributions=config['param_grid'],\n",
    "            n_iter=20,\n",
    "            cv=5,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit the search\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = search.best_estimator_\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        print(f\"{name} - Best parameters: {search.best_params_}\")\n",
    "        print(f\"{name} - Best CV score: {-search.best_score_:.4f} (RMSE)\")\n",
    "        print(f\"{name} - Test RMSE: {rmse:.4f}\")\n",
    "        print(f\"{name} - Test R2: {r2:.4f}\")\n",
    "        print(f\"{name} - Test MAE: {mae:.4f}\")\n",
    "        \n",
    "        # Save best parameters\n",
    "        with open(os.path.join(output_dir, 'results', f'{name}_best_params.txt'), 'w') as f:\n",
    "            f.write(f\"Best parameters for {name}:\\n\")\n",
    "            for param, value in search.best_params_.items():\n",
    "                f.write(f\"{param}: {value}\\n\")\n",
    "            f.write(f\"\\nCV score (RMSE): {-search.best_score_:.4f}\")\n",
    "            f.write(f\"\\nTest RMSE: {rmse:.4f}\")\n",
    "            f.write(f\"\\nTest R2: {r2:.4f}\")\n",
    "            f.write(f\"\\nTest MAE: {mae:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        tuned_models[name] = {\n",
    "            'model': best_model,\n",
    "            'best_params': search.best_params_,\n",
    "            'cv_score': -search.best_score_,\n",
    "            'test_rmse': rmse,\n",
    "            'test_r2': r2,\n",
    "            'test_mae': mae,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        # Save predictions\n",
    "        pd.DataFrame({\n",
    "            'Actual': y_test,\n",
    "            'Predicted': y_pred,\n",
    "            'Error': y_test - y_pred,\n",
    "            'Absolute Error': np.abs(y_test - y_pred)\n",
    "        }).to_csv(os.path.join(output_dir, 'results', f'{name}_predictions.csv'), index=False)\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.6, color=colors[2])\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.xlabel('Actual', fontsize=14)\n",
    "        plt.ylabel('Predicted', fontsize=14)\n",
    "        plt.title(f'{name} (Tuned): Actual vs Predicted', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), f'{name}_tuned_actual_vs_predicted.png')\n",
    "        \n",
    "        # Residual plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        residuals = y_test - y_pred\n",
    "        plt.scatter(y_pred, residuals, alpha=0.6, color=colors[3])\n",
    "        plt.axhline(y=0, color='r', linestyle='-', linewidth=1)\n",
    "        plt.xlabel('Predicted', fontsize=14)\n",
    "        plt.ylabel('Residuals', fontsize=14)\n",
    "        plt.title(f'{name} (Tuned): Residual Plot', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        save_figure(plt.gcf(), f'{name}_tuned_residuals.png')\n",
    "        \n",
    "        # Feature importance for tree-based models\n",
    "        if name in ['RandomForest', 'GradientBoosting', 'XGBoost']:\n",
    "            try:\n",
    "                # Access model from pipeline\n",
    "                model = best_model.named_steps['model']\n",
    "                \n",
    "                # Calculate permutation importance - more reliable than built-in feature importances\n",
    "                perm_importance = permutation_importance(\n",
    "                    best_model, X_test, y_test, n_repeats=10, random_state=42\n",
    "                )\n",
    "                \n",
    "                # Get feature names after preprocessing\n",
    "                if hasattr(best_model.named_steps['preprocessor'], 'transformers_'):\n",
    "                    feature_names = []\n",
    "                    for name_tr, transformer, cols in best_model.named_steps['preprocessor'].transformers_:\n",
    "                        if name_tr == 'num':\n",
    "                            feature_names.extend(cols)\n",
    "                        elif name_tr == 'cat':\n",
    "                            # For categorical features, get one-hot encoded feature names\n",
    "                            ohe = transformer\n",
    "                            if hasattr(ohe, 'get_feature_names_out'):\n",
    "                                cat_features = ohe.get_feature_names_out(cols)\n",
    "                                feature_names.extend(cat_features)\n",
    "                            else:\n",
    "                                # Fallback approach\n",
    "                                for col in cols:\n",
    "                                    unique_values = X_train[col].dropna().unique()\n",
    "                                    for val in unique_values:\n",
    "                                        feature_names.append(f\"{col}_{val}\")\n",
    "                    \n",
    "                    if feature_selector is not None:\n",
    "                        # Get the selected features indices\n",
    "                        selected_indices = best_model.named_steps['selector'].get_support()\n",
    "                        # Filter feature names to only include selected ones\n",
    "                        if len(feature_names) > len(selected_indices):\n",
    "                            feature_names = [f for i, f in enumerate(feature_names) if i < len(selected_indices) and selected_indices[i]]\n",
    "                        else:\n",
    "                            feature_names = [f for i, f in enumerate(feature_names) if i < len(selected_indices)]\n",
    "                \n",
    "                # Adjust feature names and importance to match in length                \n",
    "                min_len = min(len(feature_names), len(perm_importance.importances_mean))\n",
    "                feature_names = feature_names[:min_len]\n",
    "                importances = perm_importance.importances_mean[:min_len]\n",
    "                \n",
    "                # Create DataFrame for visualization\n",
    "                perm_importance_df = pd.DataFrame({\n",
    "                    'Feature': feature_names,\n",
    "                    'Importance': importances\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                # Save permutation importance\n",
    "                perm_importance_df.to_csv(os.path.join(output_dir, 'results', f'{name}_permutation_importance.csv'), index=False)\n",
    "                \n",
    "                # Plot top 20 features\n",
    "                plt.figure(figsize=(14, 10))\n",
    "                top_features = perm_importance_df.head(20)\n",
    "                ax = sns.barplot(x='Importance', y='Feature', data=top_features, palette='viridis')\n",
    "                ax.set_title(f'{name}: Top 20 Features by Permutation Importance', fontsize=15)\n",
    "                plt.tight_layout()\n",
    "                save_figure(plt.gcf(), f'{name}_permutation_importance.png')\n",
    "                \n",
    "                # 3. Add correct partial dependence plots for top features\n",
    "                top_feature_names = perm_importance_df.head(5)['Feature'].tolist()\n",
    "                \n",
    "                for feat_name in top_feature_names:\n",
    "                    if feat_name in feature_names:\n",
    "                        feat_idx = feature_names.index(feat_name)\n",
    "                        \n",
    "                        try:\n",
    "                            # Generate partial dependence data\n",
    "                            pd_values = partial_dependence(\n",
    "                                best_model, \n",
    "                                X_test,\n",
    "                                features=[feat_idx],\n",
    "                                kind=\"average\"\n",
    "                            )\n",
    "                            \n",
    "                            # Plot\n",
    "                            plt.figure(figsize=(10, 6))\n",
    "                            plt.plot(pd_values[\"values\"][0], pd_values[\"average\"][0], '-o')\n",
    "                            plt.xlabel(feat_name, fontsize=14)\n",
    "                            plt.ylabel(f'Partial dependence on {target}', fontsize=14)\n",
    "                            plt.title(f'Partial Dependence Plot for {feat_name}', fontsize=16)\n",
    "                            plt.grid(True, alpha=0.3)\n",
    "                            plt.tight_layout()\n",
    "                            save_figure(plt.gcf(), f'{name}_pdp_{feat_name}.png')\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: Could not generate partial dependence plot for {feat_name}: {e}\")\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not generate feature importance for {name}: {e}\")\n",
    "    \n",
    "    return tuned_models\n",
    "\n",
    "def compare_models(tuned_models):\n",
    "    \"\"\"Compare model performance\"\"\"\n",
    "    print(\"\\nComparing model performance...\")\n",
    "    \n",
    "    # Compile results\n",
    "    comparison_data = []\n",
    "    \n",
    "    for name, results in tuned_models.items():\n",
    "        comparison_data.append({\n",
    "            'Model': f\"{name} (Tuned)\",\n",
    "            'RMSE': results['test_rmse'],\n",
    "            'R2': results['test_r2'],\n",
    "            'MAE': results['test_mae'],\n",
    "            'CV_RMSE': results['cv_score']\n",
    "        })\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison = pd.DataFrame(comparison_data).sort_values('RMSE')\n",
    "    \n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(comparison)\n",
    "    \n",
    "    # Save model comparison\n",
    "    comparison.to_csv(os.path.join(output_dir, 'results', 'model_comparison.csv'), index=False)\n",
    "    \n",
    "    # Plot comparison - RMSE (lower is better)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x='Model', y='RMSE', data=comparison, palette='viridis')\n",
    "    ax.set_title('Model Comparison: RMSE (lower is better)', fontsize=16)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.set_ylabel('Root Mean Squared Error (RMSE)', fontsize=14)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, v in enumerate(comparison['RMSE']):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_figure(plt.gcf(), 'model_comparison_rmse.png')\n",
    "    \n",
    "    # Plot comparison - R2 (higher is better)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    ax = sns.barplot(x='Model', y='R2', data=comparison, palette='viridis')\n",
    "    ax.set_title('Model Comparison: R² (higher is better)', fontsize=16)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.set_ylabel('Coefficient of Determination (R²)', fontsize=14)\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, v in enumerate(comparison['R2']):\n",
    "        ax.text(i, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_figure(plt.gcf(), 'model_comparison_r2.png')\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "def generate_feature_effect_report(best_model, feature_importance, X_train, y_train, X_test, y_test, numerical_cols, categorical_cols):\n",
    "    \"\"\"Generate detailed reports on how features affect the target variable\"\"\"\n",
    "    print(\"\\nGenerating feature effect report...\")\n",
    "    \n",
    "    # Get top 10 important features\n",
    "    top_features = feature_importance.head(10)['Feature'].tolist()\n",
    "    \n",
    "    # Feature effect report\n",
    "    report = \"# Feature Effect Report\\n\\n\"\n",
    "    report += \"This report details how the top features affect the target variable '% dry after 24h in incubator'.\\n\\n\"\n",
    "    \n",
    "    # Add top features section\n",
    "    report += \"## Top 10 Most Important Features\\n\\n\"\n",
    "    report += feature_importance.head(10).to_markdown(index=False) + \"\\n\\n\"\n",
    "    \n",
    "    # Add interpretation for each category of feature\n",
    "    report += \"## Feature Interpretations\\n\\n\"\n",
    "    \n",
    "    # Group features by type\n",
    "    feature_types = {}\n",
    "    for feat in top_features:\n",
    "        if \"_x_\" in feat:\n",
    "            feature_types.setdefault(\"Interaction\", []).append(feat)\n",
    "        elif \"_to_\" in feat:\n",
    "            feature_types.setdefault(\"Ratio\", []).append(feat)\n",
    "        elif \"_squared\" in feat or \"_sqrt\" in feat:\n",
    "            feature_types.setdefault(\"Polynomial\", []).append(feat)\n",
    "        elif \"_log\" in feat:\n",
    "            feature_types.setdefault(\"Logarithmic\", []).append(feat)\n",
    "        elif \"_missing\" in feat:\n",
    "            feature_types.setdefault(\"Missing Indicator\", []).append(feat)\n",
    "        elif \"_binned\" in feat:\n",
    "            feature_types.setdefault(\"Binned\", []).append(feat)\n",
    "        elif feat in numerical_cols:\n",
    "            feature_types.setdefault(\"Numerical\", []).append(feat)\n",
    "        else:\n",
    "            feature_types.setdefault(\"Categorical\", []).append(feat)\n",
    "    \n",
    "    # Add interpretation for each feature type\n",
    "    for feat_type, feats in feature_types.items():\n",
    "        report += f\"### {feat_type} Features\\n\\n\"\n",
    "        report += f\"The following {feat_type.lower()} features are important for prediction:\\n\\n\"\n",
    "        \n",
    "        for feat in feats:\n",
    "            report += f\"- **{feat}**\"\n",
    "            \n",
    "            # Add specific interpretation based on feature type\n",
    "            if feat_type == \"Interaction\":\n",
    "                report += f\": This represents the combined effect of the constituent features. A high importance indicates that the interaction between these variables significantly affects dryness.\\n\"\n",
    "            elif feat_type == \"Ratio\":\n",
    "                report += f\": This represents the relative proportion between two features. Its importance suggests that the balance between these parameters is crucial for predicting dryness.\\n\"\n",
    "            elif feat_type == \"Polynomial\":\n",
    "                report += f\": This indicates a non-linear relationship with the target. The importance suggests that changes in this feature have an accelerating or diminishing effect on dryness.\\n\"\n",
    "            elif feat_type == \"Logarithmic\":\n",
    "                report += f\": This transformed feature captures the logarithmic relationship with the target, suggesting that proportional changes in the original feature affect dryness.\\n\"\n",
    "            elif feat_type == \"Missing Indicator\":\n",
    "                report += f\": This shows that the presence or absence of data for this feature is predictive of dryness.\\n\"\n",
    "            elif feat_type == \"Binned\":\n",
    "                report += f\": This indicates that specific ranges of this feature have distinct effects on dryness.\\n\"\n",
    "            elif feat_type == \"Numerical\":\n",
    "                report += f\": This original numerical feature directly impacts dryness outcomes.\\n\"\n",
    "            elif feat_type == \"Categorical\":\n",
    "                report += f\": Specific categories of this feature are associated with different dryness outcomes.\\n\"\n",
    "            \n",
    "        report += \"\\n\"\n",
    "    \n",
    "    # Add process parameter recommendations\n",
    "    report += \"## Process Parameter Recommendations\\n\\n\"\n",
    "    report += \"Based on the model and feature importance analysis, consider the following recommendations for optimizing the manufacturing process:\\n\\n\"\n",
    "    \n",
    "    # Extract key process parameters from top features\n",
    "    process_parameters = set()\n",
    "    for feat in top_features:\n",
    "        # Extract original parameter names from feature expressions\n",
    "        if \"_x_\" in feat:\n",
    "            parts = feat.split(\"_x_\")\n",
    "            process_parameters.update(parts)\n",
    "        elif \"_to_\" in feat:\n",
    "            parts = feat.split(\"_to_\")\n",
    "            process_parameters.update(parts)\n",
    "        elif any(suffix in feat for suffix in [\"_squared\", \"_sqrt\", \"_log\", \"_binned\", \"_missing\"]):\n",
    "            # Extract original parameter\n",
    "            base_feat = feat.split(\"_\")[0]\n",
    "            process_parameters.add(base_feat)\n",
    "        else:\n",
    "            process_parameters.add(feat)\n",
    "    \n",
    "    # Filter out feature expressions\n",
    "    process_parameters = [p for p in process_parameters if not any(x in p for x in [\"_x_\", \"_to_\", \"_squared\", \"_sqrt\", \"_log\", \"_binned\", \"_missing\"])]\n",
    "    \n",
    "    # Add recommendations for each key process parameter\n",
    "    for param in process_parameters:\n",
    "        if param in numerical_cols:\n",
    "            report += f\"- **{param}**: Monitor and control this parameter closely as it significantly affects dryness outcomes.\\n\"\n",
    "        elif param in categorical_cols:\n",
    "            report += f\"- **{param}**: Choose optimal categories for this parameter based on desired dryness outcomes.\\n\"\n",
    "    \n",
    "    # Add general recommendations\n",
    "    report += \"\\n### General Recommendations:\\n\\n\"\n",
    "    report += \"1. **Focus on Key Parameters**: Prioritize control and monitoring of the top features identified in this analysis.\\n\"\n",
    "    report += \"2. **Consider Interactions**: Pay attention to how parameters interact with each other, as these interactions are important predictors.\\n\"\n",
    "    report += \"3. **Maintain Data Quality**: Continue collecting complete data for all parameters to improve future model performance.\\n\"\n",
    "    report += \"4. **Use the Prediction Model**: Utilize the trained model to predict outcomes before conducting expensive experiments.\\n\"\n",
    "    \n",
    "    # Save the report\n",
    "    with open(os.path.join(output_dir, 'results', 'feature_effect_report.md'), 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    return report\n",
    "\n",
    "def generate_report(df, target, categorical_cols, numerical_cols, engineered_features, \n",
    "                   feature_importance, model_comparison, best_model_name, tuned_models,\n",
    "                   scaling_method, feature_selection_method):\n",
    "    \"\"\"Generate a comprehensive report\"\"\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "    # Machine Learning Analysis Report: Predicting %dry after 24h in incubator\n",
    "\n",
    "    ## Executive Summary\n",
    "    This report presents a comprehensive machine learning approach to predict the \"%dry after 24h in incubator\" metric, which is a critical quality indicator for the manufacturing process. By analyzing various input parameters and applying advanced feature engineering and modeling techniques, we've developed a model that can reliably predict this metric before conducting expensive experiments.\n",
    "\n",
    "    ## Dataset Overview\n",
    "    - Total samples: {df.shape[0]}\n",
    "    - Features used: {len(categorical_cols) + len(numerical_cols)} original + {len(engineered_features)} engineered\n",
    "    - Target variable: {target}\n",
    "    \n",
    "    ## Data Preprocessing\n",
    "    \n",
    "    ### Feature Selection\n",
    "    - Excluded UV viscosity (cP) due to high multicollinearity with Emulsion viscosity (cP)\n",
    "    - Excluded high-cardinality categorical features to prevent overfitting\n",
    "    - Removed metadata columns not relevant for prediction\n",
    "    - Separated outcome variables from input parameters\n",
    "    \n",
    "    ### Missing Value Handling\n",
    "    - Used customized imputation strategies based on missing data patterns\n",
    "    - Created missing value indicators for features with high missing rates\n",
    "    - Applied KNN imputation for complex patterns\n",
    "    \n",
    "    ### Outlier Detection\n",
    "    - Applied IQR method to identify and cap outliers\n",
    "    - Preserved data distribution while removing extreme values\n",
    "    \n",
    "    ## Feature Engineering\n",
    "    \n",
    "    ### Physical Relationships\n",
    "    - Created physically meaningful interaction features\n",
    "    - Derived process-relevant ratios (e.g., residence time, flow ratios)\n",
    "    - Generated energy density indicators\n",
    "    \n",
    "    ### Mathematical Transformations\n",
    "    - Applied logarithmic transformations to handle skewed distributions\n",
    "    - Created polynomial features to capture non-linear relationships\n",
    "    - Used binning techniques for key parameters\n",
    "    \n",
    "    ### Feature Scaling\n",
    "    - {scaling_method.title()} scaling applied to numerical features\n",
    "    \n",
    "    ### Feature Selection\n",
    "    - {feature_selection_method.title()} method used to identify most important features\n",
    "    \n",
    "    ## Most Important Features\n",
    "    Top 10 features by importance:\n",
    "    {feature_importance.head(10).to_string(index=False)}\n",
    "    \n",
    "    ## Model Performance\n",
    "    {model_comparison.to_string(index=False)}\n",
    "    \n",
    "    ### Best Model: {best_model_name}\n",
    "    \"\"\"\n",
    "    \n",
    "    if best_model_name.replace(\" (Tuned)\", \"\") in tuned_models:\n",
    "        best_params = tuned_models[best_model_name.replace(\" (Tuned)\", \"\")]['best_params']\n",
    "        report += f\"\"\"\n",
    "    Best hyperparameters:\n",
    "    {best_params}\n",
    "    \"\"\"\n",
    "    \n",
    "    report += \"\"\"\n",
    "    ## Key Findings\n",
    "    \n",
    "    1. Feature engineering significantly improved model performance, especially physically meaningful ratio features\n",
    "    2. The feature importance analysis revealed which input parameters most strongly influence the dryness outcome\n",
    "    3. Removing multicollinear features (UV viscosity) improved model robustness without losing predictive power\n",
    "    4. Process parameters related to viscosity, flow rates, and energy input are critical predictors\n",
    "    \n",
    "    ## Engineering Insights\n",
    "    \n",
    "    1. Emulsion viscosity strongly influences dryness outcomes\n",
    "    2. The ratio between core and emulsion viscosity is a key predictor\n",
    "    3. Flow rates and their ratios significantly impact product quality\n",
    "    4. Energy input relative to material flow affects curing efficiency\n",
    "    \n",
    "    ## Practical Applications\n",
    "    \n",
    "    1. Use the model to predict dryness before conducting experiments\n",
    "    2. Optimize process parameters based on feature importance analysis\n",
    "    3. Maintain tight control on the most influential parameters\n",
    "    4. Continue data collection focused on the key parameters identified\n",
    "    \n",
    "    ## Recommendations\n",
    "    \n",
    "    1. Implement the prediction model as a decision support tool for experiment planning\n",
    "    2. Design targeted experiments to verify model predictions\n",
    "    3. Optimize the manufacturing process by focusing on the most impactful parameters\n",
    "    4. Establish robust monitoring for key process variables\n",
    "    5. Continue improving the model with new experimental data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save report\n",
    "    with open(os.path.join(output_dir, 'results', 'ml_analysis_report.md'), 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"\\nReport generated: ml_analysis_report.md\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "def create_prediction_function(best_model, numerical_cols, categorical_cols):\n",
    "    \"\"\"Create a function to make predictions on new data\"\"\"\n",
    "    \n",
    "    def predict_dry_percentage(input_data):\n",
    "        \"\"\"\n",
    "        Predict the %dry after 24h in incubator based on input parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        input_data (dict): Dictionary of input parameters\n",
    "        \n",
    "        Returns:\n",
    "        float: Predicted %dry value\n",
    "        \"\"\"\n",
    "        # Convert input data to DataFrame\n",
    "        input_df = pd.DataFrame([input_data])\n",
    "        \n",
    "        # Ensure all required columns are present\n",
    "        for col in numerical_cols + categorical_cols:\n",
    "            if col not in input_df.columns:\n",
    "                raise ValueError(f\"Missing required column: {col}\")\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = best_model.predict(input_df)\n",
    "        \n",
    "        return prediction[0]\n",
    "    \n",
    "    # Print example usage\n",
    "    print(\"\\nPrediction Function Created:\")\n",
    "    print(\"Example usage:\")\n",
    "    print(\"predict_dry_percentage({\")\n",
    "    for col in numerical_cols[:3]:\n",
    "        print(f\"    '{col}': value,\")\n",
    "    for col in categorical_cols[:2]:\n",
    "        print(f\"    '{col}': 'value',\")\n",
    "    print(\"    ...\\n})\")\n",
    "    \n",
    "    # Create a sample input template to save\n",
    "    template = {}\n",
    "    for col in numerical_cols:\n",
    "        template[col] = \"ENTER_NUMERIC_VALUE\"\n",
    "    for col in categorical_cols:\n",
    "        template[col] = \"ENTER_CATEGORICAL_VALUE\"\n",
    "    \n",
    "    # Save the template to a file\n",
    "    with open(os.path.join(output_dir, 'results', 'prediction_template.json'), 'w') as f:\n",
    "        import json\n",
    "        json.dump(template, f, indent=4)\n",
    "    \n",
    "    return predict_dry_percentage\n",
    "\n",
    "def save_model_files(best_model, feature_importance, numerical_cols, categorical_cols):\n",
    "    \"\"\"Save model files for future use\"\"\"\n",
    "    import joblib\n",
    "    \n",
    "    print(\"\\nSaving model files...\")\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_model, os.path.join(output_dir, 'models', 'best_model.pkl'))\n",
    "    \n",
    "    # Save feature lists\n",
    "    with open(os.path.join(output_dir, 'models', 'model_features.json'), 'w') as f:\n",
    "        import json\n",
    "        feature_data = {\n",
    "            'numerical_columns': numerical_cols,\n",
    "            'categorical_columns': categorical_cols,\n",
    "            'important_features': feature_importance['Feature'].tolist()[:20]\n",
    "        }\n",
    "        json.dump(feature_data, f, indent=4)\n",
    "    \n",
    "    # Create a simple prediction script\n",
    "    script = \"\"\"\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load('best_model.pkl')\n",
    "\n",
    "# Load feature information\n",
    "with open('model_features.json', 'r') as f:\n",
    "    features = json.load(f)\n",
    "\n",
    "numerical_cols = features['numerical_columns']\n",
    "categorical_cols = features['categorical_columns']\n",
    "\n",
    "def predict_dry_percentage(input_data):\n",
    "    \\\"\\\"\\\"\n",
    "    Predict the %dry after 24h in incubator based on input parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    input_data (dict): Dictionary of input parameters\n",
    "    \n",
    "    Returns:\n",
    "    float: Predicted %dry value\n",
    "    \\\"\\\"\\\"\n",
    "    # Convert input data to DataFrame\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "    \n",
    "    # Ensure all required columns are present\n",
    "    for col in numerical_cols + categorical_cols:\n",
    "        if col not in input_df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_df)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    print(\"\\\\nDry Percentage Predictor\")\n",
    "    print(\"-------------------------\")\n",
    "    print(\"Enter values for the following parameters:\")\n",
    "    \n",
    "    # Create input dictionary\n",
    "    input_data = {}\n",
    "    \n",
    "    # Get numerical inputs\n",
    "    for col in numerical_cols:\n",
    "        while True:\n",
    "            try:\n",
    "                value = float(input(f\"{col}: \"))\n",
    "                input_data[col] = value\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(\"Please enter a numeric value.\")\n",
    "    \n",
    "    # Get categorical inputs\n",
    "    for col in categorical_cols:\n",
    "        input_data[col] = input(f\"{col}: \")\n",
    "    \n",
    "    # Make prediction\n",
    "    try:\n",
    "        prediction = predict_dry_percentage(input_data)\n",
    "        print(f\"\\\\nPredicted %dry after 24h in incubator: {prediction:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error making prediction: {e}\")\n",
    "\"\"\"\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'models', 'predict.py'), 'w') as f:\n",
    "        f.write(script)\n",
    "    \n",
    "    # Create a simple GUI for prediction\n",
    "    gui_script = \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import os\n",
    "\n",
    "# Load the model\n",
    "model_path = 'best_model.pkl'\n",
    "if not os.path.exists(model_path):\n",
    "    model_path = os.path.join('models', 'best_model.pkl')\n",
    "\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Load feature information\n",
    "features_path = 'model_features.json'\n",
    "if not os.path.exists(features_path):\n",
    "    features_path = os.path.join('models', 'model_features.json')\n",
    "\n",
    "with open(features_path, 'r') as f:\n",
    "    features = json.load(f)\n",
    "\n",
    "numerical_cols = features['numerical_columns']\n",
    "categorical_cols = features['categorical_columns']\n",
    "\n",
    "class PredictionApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Dry Percentage Predictor\")\n",
    "        self.root.geometry(\"800x600\")\n",
    "        \n",
    "        # Create main frame\n",
    "        main_frame = ttk.Frame(root, padding=\"10\")\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Create scrollable frame for inputs\n",
    "        canvas = tk.Canvas(main_frame)\n",
    "        scrollbar = ttk.Scrollbar(main_frame, orient=\"vertical\", command=canvas.yview)\n",
    "        self.scrollable_frame = ttk.Frame(canvas)\n",
    "        \n",
    "        self.scrollable_frame.bind(\n",
    "            \"<Configure>\",\n",
    "            lambda e: canvas.configure(scrollregion=canvas.bbox(\"all\"))\n",
    "        )\n",
    "        \n",
    "        canvas.create_window((0, 0), window=self.scrollable_frame, anchor=\"nw\")\n",
    "        canvas.configure(yscrollcommand=scrollbar.set)\n",
    "        \n",
    "        canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "        \n",
    "        # Title label\n",
    "        title_label = ttk.Label(self.scrollable_frame, \n",
    "                               text=\"Dry Percentage Predictor\", \n",
    "                               font=(\"Arial\", 16, \"bold\"))\n",
    "        title_label.grid(row=0, column=0, columnspan=2, pady=10, sticky=\"w\")\n",
    "        \n",
    "        # Description label\n",
    "        desc_label = ttk.Label(self.scrollable_frame, \n",
    "                              text=\"Enter parameter values to predict % dry after 24h in incubator\", \n",
    "                              font=(\"Arial\", 10))\n",
    "        desc_label.grid(row=1, column=0, columnspan=2, pady=5, sticky=\"w\")\n",
    "        \n",
    "        # Create input fields\n",
    "        self.input_vars = {}\n",
    "        row = 2\n",
    "        \n",
    "        # Numerical inputs\n",
    "        num_label = ttk.Label(self.scrollable_frame, \n",
    "                             text=\"Numerical Parameters\", \n",
    "                             font=(\"Arial\", 12, \"bold\"))\n",
    "        num_label.grid(row=row, column=0, columnspan=2, pady=10, sticky=\"w\")\n",
    "        row += 1\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            label = ttk.Label(self.scrollable_frame, text=f\"{col}:\")\n",
    "            label.grid(row=row, column=0, sticky=\"w\", padx=5, pady=2)\n",
    "            \n",
    "            var = tk.StringVar()\n",
    "            entry = ttk.Entry(self.scrollable_frame, textvariable=var)\n",
    "            entry.grid(row=row, column=1, sticky=\"ew\", padx=5, pady=2)\n",
    "            \n",
    "            self.input_vars[col] = var\n",
    "            row += 1\n",
    "        \n",
    "        # Categorical inputs\n",
    "        cat_label = ttk.Label(self.scrollable_frame, \n",
    "                             text=\"Categorical Parameters\", \n",
    "                             font=(\"Arial\", 12, \"bold\"))\n",
    "        cat_label.grid(row=row, column=0, columnspan=2, pady=10, sticky=\"w\")\n",
    "        row += 1\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            label = ttk.Label(self.scrollable_frame, text=f\"{col}:\")\n",
    "            label.grid(row=row, column=0, sticky=\"w\", padx=5, pady=2)\n",
    "            \n",
    "            var = tk.StringVar()\n",
    "            entry = ttk.Entry(self.scrollable_frame, textvariable=var)\n",
    "            entry.grid(row=row, column=1, sticky=\"ew\", padx=5, pady=2)\n",
    "            \n",
    "            self.input_vars[col] = var\n",
    "            row += 1\n",
    "        \n",
    "        # Predict button\n",
    "        predict_button = ttk.Button(self.scrollable_frame, \n",
    "                                   text=\"Predict\", \n",
    "                                   command=self.predict)\n",
    "        predict_button.grid(row=row, column=0, columnspan=2, pady=20)\n",
    "        row += 1\n",
    "        \n",
    "        # Result frame\n",
    "        result_frame = ttk.LabelFrame(self.scrollable_frame, text=\"Prediction Result\")\n",
    "        result_frame.grid(row=row, column=0, columnspan=2, sticky=\"ew\", padx=5, pady=10)\n",
    "        \n",
    "        self.result_var = tk.StringVar(value=\"Results will appear here\")\n",
    "        result_label = ttk.Label(result_frame, textvariable=self.result_var, font=(\"Arial\", 12))\n",
    "        result_label.pack(padx=10, pady=10)\n",
    "        \n",
    "        # Configure grid column weights\n",
    "        self.scrollable_frame.columnconfigure(1, weight=1)\n",
    "    \n",
    "    def predict(self):\n",
    "        try:\n",
    "            # Collect input values\n",
    "            input_data = {}\n",
    "            \n",
    "            # Process numerical inputs\n",
    "            for col in numerical_cols:\n",
    "                try:\n",
    "                    value = float(self.input_vars[col].get())\n",
    "                    input_data[col] = value\n",
    "                except ValueError:\n",
    "                    messagebox.showerror(\"Input Error\", f\"Please enter a valid number for {col}\")\n",
    "                    return\n",
    "            \n",
    "            # Process categorical inputs\n",
    "            for col in categorical_cols:\n",
    "                value = self.input_vars[col].get()\n",
    "                if not value.strip():\n",
    "                    messagebox.showerror(\"Input Error\", f\"Please enter a value for {col}\")\n",
    "                    return\n",
    "                input_data[col] = value\n",
    "            \n",
    "            # Make prediction\n",
    "            input_df = pd.DataFrame([input_data])\n",
    "            prediction = model.predict(input_df)[0]\n",
    "            \n",
    "            # Update result\n",
    "            self.result_var.set(f\"Predicted %dry after 24h in incubator: {prediction:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Prediction Error\", f\"Error making prediction: {str(e)}\")\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = PredictionApp(root)\n",
    "    root.mainloop()\n",
    "\"\"\"\n",
    "    \n",
    "    with open(os.path.join(output_dir, 'models', 'prediction_gui.py'), 'w') as f:\n",
    "        f.write(gui_script)\n",
    "    \n",
    "    print(\"Model files saved to the 'Predictions-Final/models' folder\")\n",
    "    print(\"- best_model.pkl: The trained model\")\n",
    "    print(\"- model_features.json: Feature information\")\n",
    "    print(\"- predict.py: Command-line prediction script\")\n",
    "    print(\"- prediction_gui.py: Simple GUI application for predictions\")\n",
    "\n",
    "def main():\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Step 1: Load and explore data\n",
    "    df, target = load_and_explore_data(file_path)\n",
    "    \n",
    "    # Step 2: Identify relevant columns (exclude metadata, outcome columns, and multicollinear features)\n",
    "    relevant_cols = identify_relevant_columns(df, target)\n",
    "    \n",
    "    # Step 3: Analyze missing value patterns\n",
    "    missing_patterns = analyze_missing_values(df, relevant_cols)\n",
    "    \n",
    "    # Step 4: Analyze features\n",
    "    categorical_cols = analyze_categorical_features(df, target, relevant_cols)\n",
    "    numerical_cols = analyze_numerical_features(df, target, relevant_cols)\n",
    "    \n",
    "    # Step 5: Filter categorical columns to avoid overfitting\n",
    "    categorical_cols = filter_categorical_columns(df, categorical_cols, threshold=10)\n",
    "    \n",
    "    # Step 6: Feature engineering\n",
    "    df_engineered, all_numerical_cols, engineered_features = feature_engineering(\n",
    "        df, numerical_cols, categorical_cols, target)\n",
    "    \n",
    "    # Step 7: Create imputation strategy with indicator variables\n",
    "    numerical_imputers, categorical_imputer, indicator_cols = create_imputation_strategy(\n",
    "        df_engineered, all_numerical_cols, categorical_cols, missing_patterns)\n",
    "    \n",
    "    # Step 8: Prepare data with comprehensive preprocessing\n",
    "    X_train, X_test, y_train, y_test, numerical_cols_with_indicators = prepare_data(\n",
    "        df_engineered, target, categorical_cols, all_numerical_cols, \n",
    "        numerical_imputers, categorical_imputer, indicator_cols)\n",
    "    \n",
    "    # Step 9: Build preprocessing pipeline with scaling options\n",
    "    scaling_method = 'standard'  # Options: 'standard', 'minmax', 'robust'\n",
    "    preprocessor = build_preprocessing_pipeline(numerical_cols_with_indicators, categorical_cols, scaling_method)\n",
    "    \n",
    "    # Step 10: Feature selection\n",
    "    feature_selection_method = 'importance'  # Options: 'importance', 'correlation', 'stability'\n",
    "    feature_selector, feature_importance, selected_features = select_features(\n",
    "        X_train, y_train, numerical_cols_with_indicators, categorical_cols, \n",
    "        preprocessor, method=feature_selection_method)\n",
    "    \n",
    "    # Step 11: Hyperparameter tuning for all models with feature selection\n",
    "    tuned_models = hyperparameter_tune(X_train, y_train, X_test, y_test, preprocessor, feature_selector)\n",
    "    \n",
    "    # Step 12: Compare models\n",
    "    model_comparison = compare_models(tuned_models)\n",
    "    \n",
    "    # Step 13: Identify best model\n",
    "    best_model_name = model_comparison.iloc[0]['Model']\n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model_name_clean = best_model_name.replace(\" (Tuned)\", \"\")\n",
    "    best_model = tuned_models[best_model_name_clean]['model']\n",
    "    \n",
    "    # Step 14: Generate feature effect report\n",
    "    feature_effect_report = generate_feature_effect_report(\n",
    "        best_model, feature_importance, X_train, y_train, X_test, y_test,\n",
    "        numerical_cols_with_indicators, categorical_cols)\n",
    "    \n",
    "    # Step 15: Create prediction function\n",
    "    prediction_function = create_prediction_function(\n",
    "        best_model, numerical_cols_with_indicators, categorical_cols)\n",
    "    \n",
    "    # Step 16: Save model files for future use\n",
    "    save_model_files(best_model, feature_importance, numerical_cols_with_indicators, categorical_cols)\n",
    "    \n",
    "    # Step 17: Generate report\n",
    "    report = generate_report(\n",
    "        df, target, categorical_cols, numerical_cols, engineered_features,\n",
    "        feature_importance, model_comparison, best_model_name, tuned_models,\n",
    "        scaling_method, feature_selection_method)\n",
    "    \n",
    "    # Calculate execution time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    hours, remainder = divmod(execution_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    print(f\"Total execution time: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    print(f\"All results saved to '{output_dir}' folder\")\n",
    "    print(\"Please check the generated report and figures for detailed analysis.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23d83b-bd22-42e2-8bd4-30597d08cdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
